{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865798ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FALSE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ==========\n",
    "# 线程超卖控制（重要）\n",
    "# ==========\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# 如果你的文件系统锁会出问题，可尝试：\n",
    "os.environ.setdefault(\"HDF5_USE_PATH_LOCKING\", \"FALSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95229431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# PATH CONFIG\n",
    "# ======================\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "ROOT_DIR = (NOTEBOOK_DIR / \"../data\").resolve()\n",
    "RUN_DIR = ROOT_DIR / \"run\"\n",
    "CWD_PATH  = RUN_DIR / \"CWD.nc\"\n",
    "dEF_PATH  = ROOT_DIR / \"ET/dEF.nc\"\n",
    "dSIF_PATH = ROOT_DIR / \"SIF/dSIF.nc\"\n",
    "\n",
    "OUT_GLOBAL_DIR = ROOT_DIR / \"veg_activity\"\n",
    "OUT_GLOBAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TILE_DIR = RUN_DIR / \"veg_activity/tiles\"\n",
    "OUT_TILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# VARIABLE NAMES\n",
    "# ======================\n",
    "VAR_CWD  = \"CWD\"\n",
    "VAR_dEF  = \"dEF\"\n",
    "VAR_dSIF = \"dSIF\"\n",
    "\n",
    "# ======================\n",
    "# TILE CONFIG\n",
    "# ======================\n",
    "TILE_Y = 300   # lat block\n",
    "TILE_X = 300   # lon block\n",
    "\n",
    "# ======================\n",
    "# METHOD PARAMS\n",
    "# ======================\n",
    "NBINS = 50\n",
    "Q = 0.90\n",
    "\n",
    "MIN_EVENT_STEPS = 1       # >=8天; 你的数据是8-day，所以1 step = 8天\n",
    "MINVALIDBINS = 8\n",
    "MINPOINTSTOTAL = 30\n",
    "\n",
    "RUNAWAY_YEARS = 5         # 连续>=5年无CWD<=0重置 -> discard pixel\n",
    "\n",
    "# 缺测值处理（你的 dEF/dSIF 是 -9999）\n",
    "FILL_X = -9999.0\n",
    "\n",
    "# 输出压缩\n",
    "ENC_2D = dict(zlib=True, complevel=4, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b0f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 828, lat: 3600, lon: 7200)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2003-01-01 2003-01-09 ... 2020-12-26\n",
      "  * lat      (lat) float32 -89.97 -89.93 -89.88 -89.82 ... 89.88 89.93 89.97\n",
      "  * lon      (lon) float32 -180.0 -179.9 -179.9 -179.8 ... 179.9 179.9 180.0\n",
      "Data variables:\n",
      "    CWD      (time, lat, lon) float32 ...\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 828, lon: 7200, lat: 3600)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2003-01-01 2003-01-09 ... 2020-12-26\n",
      "  * lon      (lon) float64 -180.0 -179.9 -179.9 -179.8 ... 179.9 179.9 180.0\n",
      "  * lat      (lat) float64 -89.97 -89.92 -89.88 -89.83 ... 89.88 89.93 89.98\n",
      "Data variables:\n",
      "    crs      |S1 ...\n",
      "    dEF      (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    CDI:                 Climate Data Interface version 2.5.0 (https://mpimet...\n",
      "    Conventions:         CF-1.5\n",
      "    GDAL_AREA_OR_POINT:  Area\n",
      "    GDAL:                GDAL 3.10.3, released 2025/04/01\n",
      "    history:             Fri Jan 09 21:33:58 2026: cdo -O -L -b F32 -f nc4 -z...\n",
      "    CDO:                 Climate Data Operators version 2.5.0 (https://mpimet...\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 828, lon: 7200, lat: 3600)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2003-01-01 2003-01-09 ... 2020-12-26\n",
      "  * lon      (lon) float64 -180.0 -179.9 -179.9 -179.8 ... 179.9 179.9 180.0\n",
      "  * lat      (lat) float64 -89.97 -89.92 -89.88 -89.83 ... 89.88 89.93 89.98\n",
      "Data variables:\n",
      "    crs      |S1 ...\n",
      "    dSIF     (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    CDI:                    Climate Data Interface version 2.5.0 (https://mpi...\n",
      "    Conventions:            CF-1.5\n",
      "    GDAL_AREA_OR_POINT:     Area\n",
      "    GDAL_TIFFTAG_SOFTWARE:  MATLAB 8.6, Mapping Toolbox 4.2\n",
      "    GDAL:                   GDAL 3.10.3, released 2025/04/01\n",
      "    history:                Fri Jan 09 21:35:29 2026: cdo -O -L -b F32 -f nc4...\n",
      "    CDO:                    Climate Data Operators version 2.5.0 (https://mpi...\n",
      "time: 2003-01-01T00:00:00.000000000 -> 2020-12-26T00:00:00.000000000 len= 828\n",
      "lat: -89.975 89.975 n= 3600\n",
      "lon: -179.975 179.975 n= 7200\n"
     ]
    }
   ],
   "source": [
    "ds_cwd  = xr.open_dataset(CWD_PATH,  engine=\"netcdf4\")\n",
    "ds_def  = xr.open_dataset(dEF_PATH,  engine=\"netcdf4\")\n",
    "ds_dsif = xr.open_dataset(dSIF_PATH, engine=\"netcdf4\")\n",
    "\n",
    "print(ds_cwd)\n",
    "print(ds_def)\n",
    "print(ds_dsif)\n",
    "\n",
    "# 取 time/lat/lon，确保一致\n",
    "time = ds_cwd[\"time\"].values\n",
    "lat  = ds_cwd[\"lat\"].values\n",
    "lon  = ds_cwd[\"lon\"].values\n",
    "\n",
    "assert ds_def[\"time\"].size == ds_cwd[\"time\"].size\n",
    "assert ds_dsif[\"time\"].size == ds_cwd[\"time\"].size\n",
    "assert ds_def[\"lat\"].size  == ds_cwd[\"lat\"].size\n",
    "assert ds_def[\"lon\"].size  == ds_cwd[\"lon\"].size\n",
    "\n",
    "print(\"time:\", time[0], \"->\", time[-1], \"len=\", len(time))\n",
    "print(\"lat:\", lat[0], lat[-1], \"n=\", len(lat))\n",
    "print(\"lon:\", lon[0], lon[-1], \"n=\", len(lon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdddb61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N years: 18 first/last: [ 0 46] [782 828]\n"
     ]
    }
   ],
   "source": [
    "def build_year_slices(time_index: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    return int32 array shape (n_years, 2) with [start, end) indices\n",
    "    \"\"\"\n",
    "    t = pd.to_datetime(time_index)\n",
    "    years = t.year.values\n",
    "    uniq_years = np.unique(years)\n",
    "    slices = []\n",
    "    for y in uniq_years:\n",
    "        idx = np.where(years == y)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        slices.append((int(idx[0]), int(idx[-1] + 1)))\n",
    "    return np.array(slices, dtype=np.int32)\n",
    "\n",
    "YEAR_SLICES = build_year_slices(time)\n",
    "print(\"N years:\", YEAR_SLICES.shape[0], \"first/last:\", YEAR_SLICES[0], YEAR_SLICES[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20af3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tiles: 288\n",
      "Example tile: (0, 300, 0, 300)\n"
     ]
    }
   ],
   "source": [
    "NY = len(lat)\n",
    "NX = len(lon)\n",
    "\n",
    "def make_tiles(ny, nx, tile_y, tile_x):\n",
    "    tiles = []\n",
    "    for y0 in range(0, ny, tile_y):\n",
    "        y1 = min(ny, y0 + tile_y)\n",
    "        for x0 in range(0, nx, tile_x):\n",
    "            x1 = min(nx, x0 + tile_x)\n",
    "            tiles.append((y0, y1, x0, x1))\n",
    "    return tiles\n",
    "\n",
    "TILES = make_tiles(NY, NX, TILE_Y, TILE_X)\n",
    "print(\"Total tiles:\", len(TILES))\n",
    "print(\"Example tile:\", TILES[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656fc5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(828, 300, 300) (828, 300, 300) (828, 300, 300)\n",
      "CWD finite ratio: 1.0\n",
      "dEF finite ratio: 0.0\n",
      "dSIF finite ratio: 3.6500268384326356e-06\n"
     ]
    }
   ],
   "source": [
    "def load_tile(y0, y1, x0, x1):\n",
    "    cwd  = ds_cwd[VAR_CWD].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load()\n",
    "    dEF  = ds_def[VAR_dEF].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load()\n",
    "    dSIF = ds_dsif[VAR_dSIF].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load()\n",
    "\n",
    "    # 缺测：-9999 -> NaN\n",
    "    dEF  = dEF.where(dEF != FILL_X)\n",
    "    dSIF = dSIF.where(dSIF != FILL_X)\n",
    "\n",
    "    # 转 float32（减内存/加速）\n",
    "    return (cwd.data.astype(np.float32),\n",
    "            dEF.data.astype(np.float32),\n",
    "            dSIF.data.astype(np.float32))\n",
    "\n",
    "# 试一个 tile\n",
    "y0,y1,x0,x1 = TILES[0]\n",
    "cwd_t, def_t, dsif_t = load_tile(y0,y1,x0,x1)\n",
    "print(cwd_t.shape, def_t.shape, dsif_t.shape)\n",
    "print(\"CWD finite ratio:\", np.isfinite(cwd_t).mean())\n",
    "print(\"dEF finite ratio:\", np.isfinite(def_t).mean())\n",
    "print(\"dSIF finite ratio:\", np.isfinite(dsif_t).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_S0.py\n",
    "# Core S0 estimation (Stocker-style binning) with lightweight QC outputs\n",
    "# - Adds: (1) slope significance (one-sided) (2) piecewise 1-change-point BIC test\n",
    "# - Returns: S0, p_one, bic_lin, bic_seg, cp, flag\n",
    "#\n",
    "# flag:\n",
    "#   0 = invalid / filtered\n",
    "#   1 = linear accepted => S0 output\n",
    "#   2 = segmented model clearly better (change point / flattening) => S0 not output\n",
    "#\n",
    "# Notes:\n",
    "# - p-value uses Normal approximation for speed & numba-compatibility\n",
    "# - piecewise model uses brute-force change point search on binned points\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _event_window_one_year(cwd_year, min_event_steps):\n",
    "    \"\"\"\n",
    "    Identify the single largest CWD event window in one year.\n",
    "    cwd_year: 1D float32, length Ty (CWD >= 0 in deficit period; reset near 0 in wet)\n",
    "    return (ok, start, end_exclusive, cwdmax)\n",
    "\n",
    "    Stocker-style trimming:\n",
    "    - peak at maximum CWD\n",
    "    - start at last CWD<=0 before peak, but we will shift +1 to avoid including reset point\n",
    "    - end at first time after peak when CWD < 0.9*CWDmax (rewetting removal)\n",
    "    \"\"\"\n",
    "    Ty = cwd_year.size\n",
    "\n",
    "    # peak (max CWD)\n",
    "    peak = -1\n",
    "    cwdmax = -1e30\n",
    "    for i in range(Ty):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v > cwdmax:\n",
    "            cwdmax = v\n",
    "            peak = i\n",
    "    if peak < 0 or (not np.isfinite(cwdmax)) or cwdmax <= 0.0:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    # start: last CWD<=0 before peak  (then +1 to exclude reset point)\n",
    "    start = 0\n",
    "    reset_idx = -1\n",
    "    for i in range(peak, -1, -1):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v <= 0.0:\n",
    "            reset_idx = i\n",
    "            break\n",
    "    if reset_idx >= 0:\n",
    "        start = reset_idx + 1\n",
    "        if start >= Ty:\n",
    "            return 0, 0, 0, 0.0\n",
    "    else:\n",
    "        # no reset found; start from 0\n",
    "        start = 0\n",
    "\n",
    "    # end: first time after peak when CWD < 0.9*CWDmax\n",
    "    thr = 0.9 * cwdmax\n",
    "    end = Ty\n",
    "    for i in range(peak, Ty):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v < thr:\n",
    "            end = i\n",
    "            break\n",
    "\n",
    "    if end - start < min_event_steps:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    return 1, start, end, cwdmax\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _count_runaway_years(cwd, year_slices):\n",
    "    \"\"\"\n",
    "    Count max consecutive years with no reset (no CWD<=0 in that year).\n",
    "    Stocker: remove pixels where accumulation extends > 5 years.\n",
    "    \"\"\"\n",
    "    nY = year_slices.shape[0]\n",
    "    max_consec = 0\n",
    "    consec = 0\n",
    "    for yi in range(nY):\n",
    "        s = year_slices[yi, 0]\n",
    "        e = year_slices[yi, 1]\n",
    "        has_reset = False\n",
    "        for t in range(s, e):\n",
    "            v = cwd[t]\n",
    "            if np.isfinite(v) and v <= 0.0:\n",
    "                has_reset = True\n",
    "                break\n",
    "        if has_reset:\n",
    "            consec = 0\n",
    "        else:\n",
    "            consec += 1\n",
    "            if consec > max_consec:\n",
    "                max_consec = consec\n",
    "    return max_consec\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Normal CDF (fast p-value approx)\n",
    "# Phi(z) = 0.5 * erfc(-z / sqrt(2))\n",
    "# -------------------------\n",
    "@nb.njit(cache=True)\n",
    "def _norm_cdf(z):\n",
    "    return 0.5 * math.erfc(-z / 1.4142135623730951)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OLS fit: y = a + b*x\n",
    "# return (ok, a, b, rss, se_b, t_b, p_one_sided)\n",
    "# p_one_sided: H1: b < 0  (Normal approx)\n",
    "# -------------------------\n",
    "@nb.njit(cache=True)\n",
    "def _ols_fit(x, y):\n",
    "    n = x.size\n",
    "    if n < 3:\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    sx = 0.0\n",
    "    sy = 0.0\n",
    "    sxx = 0.0\n",
    "    sxy = 0.0\n",
    "    for i in range(n):\n",
    "        sx += x[i]\n",
    "        sy += y[i]\n",
    "        sxx += x[i] * x[i]\n",
    "        sxy += x[i] * y[i]\n",
    "\n",
    "    denom = n * sxx - sx * sx\n",
    "    if denom == 0.0 or (not np.isfinite(denom)):\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    b = (n * sxy - sx * sy) / denom\n",
    "    a = (sy - b * sx) / n\n",
    "\n",
    "    if (not np.isfinite(a)) or (not np.isfinite(b)):\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    # RSS\n",
    "    rss = 0.0\n",
    "    for i in range(n):\n",
    "        r = y[i] - (a + b * x[i])\n",
    "        rss += r * r\n",
    "    if (not np.isfinite(rss)) or rss <= 0.0:\n",
    "        rss = 1e-30\n",
    "\n",
    "    # Standard error of b\n",
    "    xbar = sx / n\n",
    "    sxxc = 0.0\n",
    "    for i in range(n):\n",
    "        dx = x[i] - xbar\n",
    "        sxxc += dx * dx\n",
    "\n",
    "    if sxxc <= 0.0 or (not np.isfinite(sxxc)):\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    s2 = rss / (n - 2)  # residual variance\n",
    "    if (not np.isfinite(s2)) or s2 <= 0.0:\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    se_b = math.sqrt(s2 / sxxc)\n",
    "    if (not np.isfinite(se_b)) or se_b <= 0.0:\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    t_b = b / se_b\n",
    "\n",
    "    # One-sided p for H1: b < 0\n",
    "    # Normal approx\n",
    "    p_one = _norm_cdf(t_b)\n",
    "    if not np.isfinite(p_one):\n",
    "        p_one = 1.0\n",
    "\n",
    "    return 1, a, b, rss, se_b, t_b, p_one\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _bic_from_rss(rss, n, k):\n",
    "    # BIC = n * ln(RSS/n) + k * ln(n)\n",
    "    if rss <= 0.0:\n",
    "        rss = 1e-30\n",
    "    return n * math.log(rss / n) + k * math.log(n)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Best 1-change-point piecewise linear:\n",
    "# split at cp index (left: [0:cp], right: [cp:n])\n",
    "# return:\n",
    "# (ok, best_cp, rss_total, bic_seg, a1,b1, a2,b2)\n",
    "# -------------------------\n",
    "@nb.njit(cache=True)\n",
    "def _best_piecewise_1cp_bic(x, y, min_seg_points):\n",
    "    n = x.size\n",
    "    if n < 2 * min_seg_points:\n",
    "        return 0, -1, 1e30, 1e30, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    best_rss = 1e30\n",
    "    best_cp = -1\n",
    "    best_a1 = 0.0\n",
    "    best_b1 = 0.0\n",
    "    best_a2 = 0.0\n",
    "    best_b2 = 0.0\n",
    "\n",
    "    # enumerate cp positions\n",
    "    for cp in range(min_seg_points, n - min_seg_points + 1):\n",
    "        ok1, a1, b1, rss1, _, _, _ = _ols_fit(x[:cp], y[:cp])\n",
    "        if ok1 == 0:\n",
    "            continue\n",
    "        ok2, a2, b2, rss2, _, _, _ = _ols_fit(x[cp:], y[cp:])\n",
    "        if ok2 == 0:\n",
    "            continue\n",
    "        rss = rss1 + rss2\n",
    "        if rss < best_rss:\n",
    "            best_rss = rss\n",
    "            best_cp = cp\n",
    "            best_a1, best_b1 = a1, b1\n",
    "            best_a2, best_b2 = a2, b2\n",
    "\n",
    "    if best_cp < 0:\n",
    "        return 0, -1, 1e30, 1e30, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # k=5: (a1,b1,a2,b2,cp) conservative complexity\n",
    "    bic_seg = _bic_from_rss(best_rss, n, 5)\n",
    "    return 1, best_cp, best_rss, bic_seg, best_a1, best_b1, best_a2, best_b2\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Linear fit QC for S0:\n",
    "# - intercept a > 0 (recommended)\n",
    "# - slope b < 0 AND p_one < alpha\n",
    "# - |b| >= bmin to avoid exploding S0\n",
    "# returns (ok, a, b, s0, p_one, bic_lin)\n",
    "# -------------------------\n",
    "@nb.njit(cache=True)\n",
    "def _linfit_s0_qc(x, y, alpha, bmin):\n",
    "    ok, a, b, rss, se_b, t_b, p_one = _ols_fit(x, y)\n",
    "    if ok == 0:\n",
    "        return 0, 0.0, 0.0, np.nan, 1.0, 1e30\n",
    "\n",
    "    n = x.size\n",
    "    bic_lin = _bic_from_rss(rss, n, 2)\n",
    "\n",
    "    # QC: intercept\n",
    "    if (not np.isfinite(a)) or a <= 0.0:\n",
    "        return 0, a, b, np.nan, p_one, bic_lin\n",
    "\n",
    "    # QC: slope direction & magnitude\n",
    "    if (not np.isfinite(b)) or b >= 0.0:\n",
    "        return 0, a, b, np.nan, p_one, bic_lin\n",
    "    if abs(b) < bmin:\n",
    "        return 0, a, b, np.nan, p_one, bic_lin\n",
    "\n",
    "    # QC: significance (one-sided)\n",
    "    if (not np.isfinite(p_one)) or (p_one >= alpha):\n",
    "        return 0, a, b, np.nan, p_one, bic_lin\n",
    "\n",
    "    s0 = -a / b\n",
    "    if (not np.isfinite(s0)) or (s0 <= 0.0):\n",
    "        return 0, a, b, np.nan, p_one, bic_lin\n",
    "\n",
    "    return 1, a, b, s0, p_one, bic_lin\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _s0_one_pixel_qc(\n",
    "    cwd, X, year_slices, nbins, q,\n",
    "    min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "    alpha, bmin, min_seg_points, delta_bic\n",
    "):\n",
    "    \"\"\"\n",
    "    One pixel S0 with QC outputs.\n",
    "    Returns:\n",
    "      s0(float32), p_one(float32), bic_lin(float32), bic_seg(float32), cp(int16), flag(uint8)\n",
    "\n",
    "    flag:\n",
    "      0 invalid/filtered\n",
    "      1 linear accepted => S0 output\n",
    "      2 segmented better (change point / flattening) => S0 not output\n",
    "    \"\"\"\n",
    "    # runaway check\n",
    "    if _count_runaway_years(cwd, year_slices) >= runaway_years:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    T = cwd.size\n",
    "    nY = year_slices.shape[0]\n",
    "\n",
    "    # pooling arrays (max T points)\n",
    "    pool_cwd = np.empty(T, dtype=np.float32)\n",
    "    pool_x   = np.empty(T, dtype=np.float32)\n",
    "    n_pool = 0\n",
    "    gmax = 0.0\n",
    "\n",
    "    # per-year largest event pooling\n",
    "    for yi in range(nY):\n",
    "        s = year_slices[yi, 0]\n",
    "        e = year_slices[yi, 1]\n",
    "        ok, st, ed, cwdmax = _event_window_one_year(cwd[s:e], min_event_steps)\n",
    "        if ok == 0:\n",
    "            continue\n",
    "\n",
    "        # Add points from window\n",
    "        for tt in range(s + st, s + ed):\n",
    "            c = cwd[tt]\n",
    "            x = X[tt]\n",
    "            # key: only use c>0 to avoid reset/wet points\n",
    "            if np.isfinite(c) and np.isfinite(x) and c > 0.0:\n",
    "                pool_cwd[n_pool] = c\n",
    "                pool_x[n_pool] = x\n",
    "                n_pool += 1\n",
    "                if c > gmax:\n",
    "                    gmax = c\n",
    "\n",
    "    if n_pool < min_points_total or (not np.isfinite(gmax)) or gmax <= 0.0:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    # ===== binning: count -> regroup X by bin -> sort -> quantile\n",
    "    counts = np.zeros(nbins, dtype=np.int32)\n",
    "    bin_id = np.empty(n_pool, dtype=np.int32)\n",
    "\n",
    "    for i in range(n_pool):\n",
    "        b = int(pool_cwd[i] / gmax * nbins)\n",
    "        if b < 0:\n",
    "            b = 0\n",
    "        elif b >= nbins:\n",
    "            b = nbins - 1\n",
    "        bin_id[i] = b\n",
    "        counts[b] += 1\n",
    "\n",
    "    offsets = np.empty(nbins, dtype=np.int32)\n",
    "    pos = 0\n",
    "    for b in range(nbins):\n",
    "        offsets[b] = pos\n",
    "        pos += counts[b]\n",
    "\n",
    "    xgrp = np.empty(n_pool, dtype=np.float32)\n",
    "    cursor = offsets.copy()\n",
    "    for i in range(n_pool):\n",
    "        b = bin_id[i]\n",
    "        p = cursor[b]\n",
    "        xgrp[p] = pool_x[i]\n",
    "        cursor[b] += 1\n",
    "\n",
    "    xb = np.empty(nbins, dtype=np.float32)\n",
    "    yqv = np.empty(nbins, dtype=np.float32)\n",
    "    n_valid = 0\n",
    "\n",
    "    for b in range(nbins):\n",
    "        n = counts[b]\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        st = offsets[b]\n",
    "        ed = st + n\n",
    "\n",
    "        xgrp[st:ed].sort()\n",
    "\n",
    "        k = int(math.ceil(q * n) - 1)\n",
    "        if k < 0:\n",
    "            k = 0\n",
    "        if k >= n:\n",
    "            k = n - 1\n",
    "\n",
    "        xq = xgrp[st + k]\n",
    "        if not np.isfinite(xq):\n",
    "            continue\n",
    "\n",
    "        center = (b + 0.5) / nbins * gmax\n",
    "        xb[n_valid] = center\n",
    "        yqv[n_valid] = xq\n",
    "        n_valid += 1\n",
    "\n",
    "    if n_valid < min_valid_bins:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    x_use = xb[:n_valid]\n",
    "    y_use = yqv[:n_valid]\n",
    "\n",
    "    # -------- (1) Linear fit QC --------\n",
    "    ok, a, b, s0, p_one, bic_lin = _linfit_s0_qc(x_use, y_use, alpha, bmin)\n",
    "    if ok == 0:\n",
    "        # still return p_one & bic_lin if possible: do raw OLS once\n",
    "        ok0, a0, b0, rss0, se_b0, t_b0, p0 = _ols_fit(x_use, y_use)\n",
    "        if ok0 == 1:\n",
    "            bic0 = _bic_from_rss(rss0, n_valid, 2)\n",
    "            return np.float32(np.nan), np.float32(p0), np.float32(bic0), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    # -------- (2) Piecewise 1 change-point BIC test --------\n",
    "    ok2, cp, rss_seg, bic_seg, a1, b1, a2, b2 = _best_piecewise_1cp_bic(x_use, y_use, min_seg_points)\n",
    "    if ok2 == 1:\n",
    "        # if segmented clearly better: classify as change point / flattening => do not output S0\n",
    "        # condition: bic_seg + delta_bic < bic_lin  (bic_seg substantially smaller)\n",
    "        if bic_seg + delta_bic < bic_lin:\n",
    "            return np.float32(np.nan), np.float32(p_one), np.float32(bic_lin), np.float32(bic_seg), np.int16(cp), np.uint8(2)\n",
    "        else:\n",
    "            return np.float32(s0), np.float32(p_one), np.float32(bic_lin), np.float32(bic_seg), np.int16(cp), np.uint8(1)\n",
    "\n",
    "    # if no valid segmented fit\n",
    "    return np.float32(s0), np.float32(p_one), np.float32(bic_lin), np.float32(np.nan), np.int16(-1), np.uint8(1)\n",
    "\n",
    "\n",
    "@nb.njit(parallel=True, cache=True)\n",
    "def s0_tile_qc(\n",
    "    cwd3d, X3d, year_slices, nbins, q,\n",
    "    min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "    alpha, bmin, min_seg_points, delta_bic\n",
    "):\n",
    "    \"\"\"\n",
    "    cwd3d, X3d: (T, Y, X)\n",
    "\n",
    "    Returns:\n",
    "      s0_2d      float32 (Y, X)  (NaN if filtered)\n",
    "      p_one_2d   float32 (Y, X)\n",
    "      bic_lin_2d float32 (Y, X)\n",
    "      bic_seg_2d float32 (Y, X)\n",
    "      cp_2d      int16   (Y, X)\n",
    "      flag_2d    uint8   (Y, X)\n",
    "    \"\"\"\n",
    "    T, Y, X = cwd3d.shape\n",
    "\n",
    "    s0_out = np.empty((Y, X), dtype=np.float32)\n",
    "    p_out  = np.empty((Y, X), dtype=np.float32)\n",
    "    bl_out = np.empty((Y, X), dtype=np.float32)\n",
    "    bs_out = np.empty((Y, X), dtype=np.float32)\n",
    "    cp_out = np.empty((Y, X), dtype=np.int16)\n",
    "    fg_out = np.empty((Y, X), dtype=np.uint8)\n",
    "\n",
    "    n_pix = Y * X\n",
    "    for p in nb.prange(n_pix):\n",
    "        j = p // X\n",
    "        i = p - j * X\n",
    "        s0, p_one, bic_lin, bic_seg, cp, flag = _s0_one_pixel_qc(\n",
    "            cwd3d[:, j, i], X3d[:, j, i],\n",
    "            year_slices, nbins, q,\n",
    "            min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "            alpha, bmin, min_seg_points, delta_bic\n",
    "        )\n",
    "        s0_out[j, i] = s0\n",
    "        p_out[j, i]  = p_one\n",
    "        bl_out[j, i] = bic_lin\n",
    "        bs_out[j, i] = bic_seg\n",
    "        cp_out[j, i] = cp\n",
    "        fg_out[j, i] = flag\n",
    "\n",
    "    return s0_out, p_out, bl_out, bs_out, cp_out, fg_out\n",
    "\n",
    "\n",
    "# Backward-compatible: original behavior (S0 only)\n",
    "@nb.njit(parallel=True, cache=True)\n",
    "def s0_tile(\n",
    "    cwd3d, X3d, year_slices, nbins, q,\n",
    "    min_event_steps, min_valid_bins, min_points_total, runaway_years\n",
    "):\n",
    "    \"\"\"\n",
    "    Keeps original signature, returns only S0 (Y, X).\n",
    "    Uses default QC hyperparameters suitable for EF/SIF_norm bin regression.\n",
    "    \"\"\"\n",
    "    # Default QC params (tune as needed):\n",
    "    alpha = 0.05\n",
    "    bmin = 1e-8          # tune for your X scale\n",
    "    min_seg_points = 5   # minimum bins per segment\n",
    "    delta_bic = 2.0      # BIC improvement threshold\n",
    "\n",
    "    s0_out, p_out, bl_out, bs_out, cp_out, fg_out = s0_tile_qc(\n",
    "        cwd3d, X3d, year_slices, nbins, q,\n",
    "        min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "        alpha, bmin, min_seg_points, delta_bic\n",
    "    )\n",
    "    return s0_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72232fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SdEF tile: nan nan nan% 1.0\n",
      "SdSIF tile: nan nan nan% 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3336617/670419628.py:12: RuntimeWarning: All-NaN slice encountered\n",
      "  print(\"SdEF tile:\", np.nanmin(s0_def), np.nanmax(s0_def), \"nan%\", np.isnan(s0_def).mean())\n",
      "/tmp/ipykernel_3336617/670419628.py:13: RuntimeWarning: All-NaN slice encountered\n",
      "  print(\"SdSIF tile:\", np.nanmin(s0_dsif), np.nanmax(s0_dsif), \"nan%\", np.isnan(s0_dsif).mean())\n"
     ]
    }
   ],
   "source": [
    "# 选一个 tile\n",
    "y0,y1,x0,x1 = TILES[0]\n",
    "cwd_t, def_t, dsif_t = load_tile(y0,y1,x0,x1)\n",
    "\n",
    "# 运行 numba（第一次会编译，稍慢；后面很快）\n",
    "s0_def  = s0_tile(cwd_t, def_t, YEAR_SLICES, NBINS, Q,\n",
    "                  MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS)\n",
    "\n",
    "s0_dsif = s0_tile(cwd_t, dsif_t, YEAR_SLICES, NBINS, Q,\n",
    "                  MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS)\n",
    "\n",
    "print(\"SdEF tile:\", np.nanmin(s0_def), np.nanmax(s0_def), \"nan%\", np.isnan(s0_def).mean())\n",
    "print(\"SdSIF tile:\", np.nanmin(s0_dsif), np.nanmax(s0_dsif), \"nan%\", np.isnan(s0_dsif).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /tera04/zhwei/xionghui/bedrock/data/out_tiles/SdEF_y0000-0300_x0000-0300.nc4 /tera04/zhwei/xionghui/bedrock/data/out_tiles/SdSIF_y0000-0300_x0000-0300.nc4\n"
     ]
    }
   ],
   "source": [
    "def write_tile(y0,y1,x0,x1, s0_def, s0_dsif):\n",
    "    lat_tile = lat[y0:y1]\n",
    "    lon_tile = lon[x0:x1]\n",
    "\n",
    "    da_def = xr.DataArray(s0_def, dims=(\"lat\",\"lon\"),\n",
    "                          coords={\"lat\": lat_tile, \"lon\": lon_tile},\n",
    "                          name=\"SdEF\").astype(\"float32\")\n",
    "    da_sif = xr.DataArray(s0_dsif, dims=(\"lat\",\"lon\"),\n",
    "                          coords={\"lat\": lat_tile, \"lon\": lon_tile},\n",
    "                          name=\"SdSIF\").astype(\"float32\")\n",
    "\n",
    "    tile_tag = f\"y{y0:04d}-{y1:04d}_x{x0:04d}-{x1:04d}\"\n",
    "    f_def = OUT_TILE_DIR / f\"SdEF_{tile_tag}.nc\"\n",
    "    f_sif = OUT_TILE_DIR / f\"SdSIF_{tile_tag}.nc\"\n",
    "\n",
    "    xr.Dataset({\"SdEF\": da_def}).to_netcdf(f_def, engine=\"netcdf4\", format=\"NETCDF4\",\n",
    "                                          encoding={\"SdEF\": ENC_2D})\n",
    "    xr.Dataset({\"SdSIF\": da_sif}).to_netcdf(f_sif, engine=\"netcdf4\", format=\"NETCDF4\",\n",
    "                                            encoding={\"SdSIF\": ENC_2D})\n",
    "\n",
    "    return f_def, f_sif\n",
    "\n",
    "f1, f2 = write_tile(y0,y1,x0,x1, s0_def, s0_dsif)\n",
    "print(\"Wrote:\", f1, f2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484252d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] 10/288 done, fail=0\n",
      "[PROGRESS] 20/288 done, fail=0\n",
      "[PROGRESS] 30/288 done, fail=0\n",
      "[PROGRESS] 40/288 done, fail=0\n",
      "[PROGRESS] 50/288 done, fail=0\n",
      "[PROGRESS] 60/288 done, fail=0\n",
      "[PROGRESS] 70/288 done, fail=0\n",
      "[PROGRESS] 80/288 done, fail=0\n",
      "[PROGRESS] 90/288 done, fail=0\n",
      "[PROGRESS] 100/288 done, fail=0\n",
      "[PROGRESS] 110/288 done, fail=0\n",
      "[PROGRESS] 120/288 done, fail=0\n",
      "[PROGRESS] 130/288 done, fail=0\n",
      "[PROGRESS] 140/288 done, fail=0\n",
      "[PROGRESS] 150/288 done, fail=0\n",
      "[PROGRESS] 160/288 done, fail=0\n",
      "[PROGRESS] 170/288 done, fail=0\n",
      "[PROGRESS] 180/288 done, fail=0\n",
      "[PROGRESS] 190/288 done, fail=0\n",
      "[PROGRESS] 200/288 done, fail=0\n",
      "[PROGRESS] 210/288 done, fail=0\n",
      "[PROGRESS] 220/288 done, fail=0\n",
      "[PROGRESS] 230/288 done, fail=0\n",
      "[PROGRESS] 240/288 done, fail=0\n",
      "[PROGRESS] 250/288 done, fail=0\n",
      "[PROGRESS] 260/288 done, fail=0\n",
      "[PROGRESS] 270/288 done, fail=0\n",
      "[PROGRESS] 280/288 done, fail=0\n",
      "All done. fail= 0\n"
     ]
    }
   ],
   "source": [
    "def worker_one_tile(tile_id):\n",
    "    y0,y1,x0,x1 = TILES[tile_id]\n",
    "    tile_tag = f\"y{y0:04d}-{y1:04d}_x{x0:04d}-{x1:04d}\"\n",
    "    f_def = OUT_TILE_DIR / f\"SdEF_{tile_tag}.nc\"\n",
    "    f_sif = OUT_TILE_DIR / f\"SdSIF_{tile_tag}.nc\"\n",
    "\n",
    "    # checkpoint: 已存在则跳过\n",
    "    if f_def.exists() and f_sif.exists():\n",
    "        return tile_id, \"SKIP\"\n",
    "\n",
    "    # 每个进程独立打开（关键）\n",
    "    ds_c = xr.open_dataset(CWD_PATH, engine=\"netcdf4\")\n",
    "    ds_e = xr.open_dataset(dEF_PATH, engine=\"netcdf4\")\n",
    "    ds_s = xr.open_dataset(dSIF_PATH, engine=\"netcdf4\")\n",
    "\n",
    "    cwd  = ds_c[VAR_CWD].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load().data.astype(np.float32)\n",
    "    dEF  = ds_e[VAR_dEF].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load()\n",
    "    dSIF = ds_s[VAR_dSIF].isel(lat=slice(y0,y1), lon=slice(x0,x1)).transpose(\"time\",\"lat\",\"lon\").load()\n",
    "\n",
    "    dEF  = dEF.where(dEF != FILL_X).data.astype(np.float32)\n",
    "    dSIF = dSIF.where(dSIF != FILL_X).data.astype(np.float32)\n",
    "\n",
    "    # compute\n",
    "    s0_def  = s0_tile(cwd, dEF,  YEAR_SLICES, NBINS, Q,\n",
    "                      MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS)\n",
    "    s0_dsif = s0_tile(cwd, dSIF, YEAR_SLICES, NBINS, Q,\n",
    "                      MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS)\n",
    "\n",
    "    # write\n",
    "    lat_tile = lat[y0:y1]\n",
    "    lon_tile = lon[x0:x1]\n",
    "\n",
    "    da_def = xr.DataArray(s0_def, dims=(\"lat\",\"lon\"),\n",
    "                          coords={\"lat\": lat_tile, \"lon\": lon_tile},\n",
    "                          name=\"SdEF\").astype(\"float32\")\n",
    "    da_sif = xr.DataArray(s0_dsif, dims=(\"lat\",\"lon\"),\n",
    "                          coords={\"lat\": lat_tile, \"lon\": lon_tile},\n",
    "                          name=\"SdSIF\").astype(\"float32\")\n",
    "\n",
    "    xr.Dataset({\"SdEF\": da_def}).to_netcdf(f_def, engine=\"netcdf4\", format=\"NETCDF4\",\n",
    "                                           encoding={\"SdEF\": ENC_2D})\n",
    "    xr.Dataset({\"SdSIF\": da_sif}).to_netcdf(f_sif, engine=\"netcdf4\", format=\"NETCDF4\",\n",
    "                                            encoding={\"SdSIF\": ENC_2D})\n",
    "    return tile_id, \"OK\"\n",
    "\n",
    "\n",
    "# ====== 并行执行 ======\n",
    "N_WORKERS = 24  # 你说至少24进程\n",
    "\n",
    "done = 0\n",
    "fail = 0\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "    futs = {ex.submit(worker_one_tile, tid): tid for tid in range(len(TILES))}\n",
    "    for fut in as_completed(futs):\n",
    "        tid = futs[fut]\n",
    "        try:\n",
    "            tid2, status = fut.result()\n",
    "            done += 1\n",
    "            if status != \"OK\" and status != \"SKIP\":\n",
    "                fail += 1\n",
    "            if done % 10 == 0:\n",
    "                print(f\"[PROGRESS] {done}/{len(TILES)} done, fail={fail}\")\n",
    "        except Exception as e:\n",
    "            fail += 1\n",
    "            print(f\"[FAIL] tile {tid}:\", repr(e))\n",
    "\n",
    "print(\"All done. fail=\", fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "493ef376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote global: /tera04/zhwei/xionghui/bedrock/data/veg_activity/SdEF.nc /tera04/zhwei/xionghui/bedrock/data/veg_activity/SdSIF.nc\n"
     ]
    }
   ],
   "source": [
    "def assemble_global(varname, pattern_prefix, out_path):\n",
    "    files = sorted(OUT_TILE_DIR.glob(f\"{pattern_prefix}_y*-*_x*-*.nc\"))\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"No tile files found for \" + pattern_prefix)\n",
    "\n",
    "    # open_mfdataset 自动按坐标拼接（要求 tile 坐标不重叠且完整）\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", engine=\"netcdf4\")\n",
    "    da = ds[varname].astype(\"float32\")\n",
    "\n",
    "    # 输出 chunk 可自行调\n",
    "    enc = {varname: dict(zlib=True, complevel=4, dtype=\"float32\", chunksizes=(600,600))}\n",
    "    xr.Dataset({varname: da}).to_netcdf(out_path, engine=\"netcdf4\", format=\"NETCDF4\", encoding=enc)\n",
    "    return out_path\n",
    "\n",
    "out_def = assemble_global(\"SdEF\", \"SdEF\", OUT_GLOBAL_DIR / \"SdEF.nc\")\n",
    "out_sif = assemble_global(\"SdSIF\", \"SdSIF\", OUT_GLOBAL_DIR / \"SdSIF.nc\")\n",
    "print(\"Wrote global:\", out_def, out_sif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c997d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"SdEF\": {\n",
      "    \"nan_frac\": 0.8817354938271605,\n",
      "    \"p01\": 25.294102001190186,\n",
      "    \"p50\": 324.04734802246094,\n",
      "    \"p99\": 24972.2095703125,\n",
      "    \"min\": 0.004365758970379829,\n",
      "    \"max\": 293556928.0\n",
      "  },\n",
      "  \"SdSIF\": {\n",
      "    \"nan_frac\": 0.8932793981481482,\n",
      "    \"p01\": 11.527358474731447,\n",
      "    \"p50\": 348.12701416015625,\n",
      "    \"p99\": 19929.036035156165,\n",
      "    \"min\": 0.0007142720278352499,\n",
      "    \"max\": 477670848.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ds1 = xr.open_dataset(out_def)\n",
    "ds2 = xr.open_dataset(out_sif)\n",
    "\n",
    "SdEF  = ds1[\"SdEF\"].values\n",
    "SdSIF = ds2[\"SdSIF\"].values\n",
    "\n",
    "qc = {\n",
    "    \"SdEF\": {\n",
    "        \"nan_frac\": float(np.isnan(SdEF).mean()),\n",
    "        \"p01\": float(np.nanpercentile(SdEF, 1)),\n",
    "        \"p50\": float(np.nanpercentile(SdEF, 50)),\n",
    "        \"p99\": float(np.nanpercentile(SdEF, 99)),\n",
    "        \"min\": float(np.nanmin(SdEF)),\n",
    "        \"max\": float(np.nanmax(SdEF)),\n",
    "    },\n",
    "    \"SdSIF\": {\n",
    "        \"nan_frac\": float(np.isnan(SdSIF).mean()),\n",
    "        \"p01\": float(np.nanpercentile(SdSIF, 1)),\n",
    "        \"p50\": float(np.nanpercentile(SdSIF, 50)),\n",
    "        \"p99\": float(np.nanpercentile(SdSIF, 99)),\n",
    "        \"min\": float(np.nanmin(SdSIF)),\n",
    "        \"max\": float(np.nanmax(SdSIF)),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(qc, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
