{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs\n",
    "# -----------------------------\n",
    "PATH_CWD  = \"/share/home/dq076/bedrock/data/CWD/CWD.nc\"\n",
    "PATH_dEF  = \"/share/home/dq076/bedrock/data/dEF/dEF.nc\"\n",
    "PATH_dSIF = \"/share/home/dq076/bedrock/data/dSIF/dSIF.nc\"\n",
    "\n",
    "OUT_SdEF  = \"/share/home/dq076/bedrock/data/S0/SdEF.nc4\"\n",
    "OUT_SdSIF = \"/share/home/dq076/bedrock/data/S0/SdSIF.nc4\"\n",
    "\n",
    "# -----------------------------\n",
    "# Performance tuning (important)\n",
    "# -----------------------------\n",
    "CHUNK = {\"time\": -1, \"lat\": 200, \"lon\": 200}\n",
    "N_BINS = 15\n",
    "Q = 0.90\n",
    "MIN_VALID_BINS = 8\n",
    "ZERO_EPS = 1e-6   # treat CWD <= ZERO_EPS as 0\n",
    "MIN_POINTS_TOTAL = 30  # pooled (CWD,X) points needed to be meaningful, adjust if needed\n",
    "\n",
    "# -----------------------------\n",
    "# Read (lazy + chunked)\n",
    "# -----------------------------\n",
    "ds_cwd  = xr.open_dataset(PATH_CWD,  chunks=CHUNK)\n",
    "ds_def  = xr.open_dataset(PATH_dEF,  chunks=CHUNK)\n",
    "ds_dsif = xr.open_dataset(PATH_dSIF, chunks=CHUNK)\n",
    "\n",
    "cwd  = ds_cwd[\"CWD\"].astype(\"float32\")\n",
    "dEF  = ds_def[\"dEF\"].astype(\"float32\")\n",
    "dSIF = ds_dsif[\"dSIF\"].astype(\"float32\")\n",
    "\n",
    "# Force exact alignment (will raise if mismatch)\n",
    "cwd, dEF, dSIF = xr.align(cwd, dEF, dSIF, join=\"exact\")\n",
    "\n",
    "# Ensure dimension order\n",
    "cwd  = cwd.transpose(\"time\", \"lat\", \"lon\")\n",
    "dEF  = dEF.transpose(\"time\", \"lat\", \"lon\")\n",
    "dSIF = dSIF.transpose(\"time\", \"lat\", \"lon\")\n",
    "\n",
    "# Build year slices from time coordinate (assumes contiguous time axis)\n",
    "years = cwd[\"time\"].dt.year.values.astype(np.int32)\n",
    "uniq_years = np.unique(years)\n",
    "\n",
    "year_slices = []\n",
    "for y in uniq_years:\n",
    "    idx = np.where(years == y)[0]\n",
    "    if idx.size > 0:\n",
    "        year_slices.append((y, int(idx[0]), int(idx[-1] + 1)))  # [start, end)\n",
    "year_slices_arr = np.array(year_slices, dtype=np.int32)  # (Ny, 3)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Numba kernels (UPDATED to pooled-across-years binning, per your summary)\n",
    "# =====================================================================\n",
    "\n",
    "@nb.njit\n",
    "def _linfit_s0(x, y):\n",
    "    \"\"\"Fit y = a + b*x, return S0=-a/b. Use only finite points.\"\"\"\n",
    "    n = 0\n",
    "    sx = sy = sxx = sxy = 0.0\n",
    "    for i in range(x.size):\n",
    "        xi = x[i]\n",
    "        yi = y[i]\n",
    "        if np.isfinite(xi) and np.isfinite(yi):\n",
    "            n += 1\n",
    "            sx += xi\n",
    "            sy += yi\n",
    "            sxx += xi * xi\n",
    "            sxy += xi * yi\n",
    "\n",
    "    if n < MIN_VALID_BINS:\n",
    "        return np.nan\n",
    "\n",
    "    denom = n * sxx - sx * sx\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    b = (n * sxy - sx * sy) / denom\n",
    "    a = (sy - b * sx) / n\n",
    "\n",
    "    # paper logic: slope should be negative (water stress reduces X)\n",
    "    if not np.isfinite(a) or not np.isfinite(b) or b >= 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    s0 = -a / b\n",
    "    if not np.isfinite(s0) or s0 <= 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    return s0\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def _event_window_one_year(cwd_y):\n",
    "    \"\"\"\n",
    "    Given one-year CWD series (T,), return (ok, start, end, cwdmax).\n",
    "    Event definition (your summary + my correction):\n",
    "      - peak = argmax(CWD)\n",
    "      - start = first index after last CWD<=0 before peak\n",
    "      - end   = first index after peak where CWD < 0.9*CWDmax (exclude end and beyond)\n",
    "    \"\"\"\n",
    "    T = cwd_y.size\n",
    "    if T < 6:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    # peak\n",
    "    peak = 0\n",
    "    cwdmax = -1.0\n",
    "    for t in range(T):\n",
    "        v = cwd_y[t]\n",
    "        if np.isfinite(v) and v > cwdmax:\n",
    "            cwdmax = v\n",
    "            peak = t\n",
    "    if (not np.isfinite(cwdmax)) or cwdmax <= 0.0:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    # start: last ~0 before peak\n",
    "    start = 0\n",
    "    for t in range(peak, -1, -1):\n",
    "        v = cwd_y[t]\n",
    "        if np.isfinite(v) and v <= ZERO_EPS:\n",
    "            start = t + 1\n",
    "            break\n",
    "\n",
    "    # end: first after peak where CWD drops below 0.9*cwdmax\n",
    "    thr = 0.9 * cwdmax\n",
    "    end = T\n",
    "    for t in range(peak + 1, T):\n",
    "        v = cwd_y[t]\n",
    "        if np.isfinite(v) and v < thr:\n",
    "            end = t\n",
    "            break\n",
    "\n",
    "    if end - start < 6:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    return 1, start, end, cwdmax\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def _s0_pooled_one_pixel(cwd_ts, x_ts, year_slices_arr):\n",
    "    \"\"\"\n",
    "    Correct method for ONE pixel:\n",
    "      - For each year: pick max-CWD event window (start:end) using CWD only\n",
    "      - Pool all (CWD, X) points from all selected yearly events\n",
    "      - Define bins using GLOBAL max CWD across pooled points (i.e., across years)\n",
    "      - For each bin: compute X 90% quantile\n",
    "      - Regress Xq90 vs bin-center CWD => S0=-a/b\n",
    "    \"\"\"\n",
    "    Ny = year_slices_arr.shape[0]\n",
    "    T_all = cwd_ts.size  # up to 828\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Pass 1: find global max CWD across ALL yearly max events\n",
    "    # ---------------------------------------------------------\n",
    "    global_max = -1.0\n",
    "    total_points = 0\n",
    "\n",
    "    for k in range(Ny):\n",
    "        st = year_slices_arr[k, 1]\n",
    "        ed = year_slices_arr[k, 2]\n",
    "        ok, s, e, cwdmax = _event_window_one_year(cwd_ts[st:ed])\n",
    "        if ok == 0:\n",
    "            continue\n",
    "        # update global max from this year max\n",
    "        if cwdmax > global_max:\n",
    "            global_max = cwdmax\n",
    "        total_points += (e - s)\n",
    "\n",
    "    if (not np.isfinite(global_max)) or global_max <= 0.0:\n",
    "        return np.nan\n",
    "    if total_points < MIN_POINTS_TOTAL:\n",
    "        return np.nan\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Pass 2: bin pooled points (store X values per bin)\n",
    "    #   bin range: [0, global_max], equal-width N_BINS\n",
    "    # ---------------------------------------------------------\n",
    "    # We store X values in bin_vals[bin, idx], idx up to total_points (<=828)\n",
    "    bin_vals = np.empty((N_BINS, T_all), dtype=np.float32)\n",
    "    bin_cnt  = np.zeros(N_BINS, dtype=np.int32)\n",
    "\n",
    "    for k in range(Ny):\n",
    "        st = year_slices_arr[k, 1]\n",
    "        ed = year_slices_arr[k, 2]\n",
    "        cwd_y = cwd_ts[st:ed]\n",
    "        x_y   = x_ts[st:ed]\n",
    "\n",
    "        ok, s, e, _ = _event_window_one_year(cwd_y)\n",
    "        if ok == 0:\n",
    "            continue\n",
    "\n",
    "        for t in range(s, e):\n",
    "            c = cwd_y[t]\n",
    "            v = x_y[t]\n",
    "            if not (np.isfinite(c) and np.isfinite(v)):\n",
    "                continue\n",
    "            if c < 0.0:\n",
    "                continue\n",
    "\n",
    "            # bin index by GLOBAL max\n",
    "            bi = int(np.floor((c / global_max) * (N_BINS - 1) + 1e-8))\n",
    "            if bi < 0:\n",
    "                bi = 0\n",
    "            elif bi >= N_BINS:\n",
    "                bi = N_BINS - 1\n",
    "\n",
    "            kk = bin_cnt[bi]\n",
    "            bin_vals[bi, kk] = v\n",
    "            bin_cnt[bi] = kk + 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Quantile per bin -> regression arrays\n",
    "    #   CWD coordinate for each bin = bin center (NOT the CWD of the chosen X point)\n",
    "    # ---------------------------------------------------------\n",
    "    x_fit = np.empty(N_BINS, dtype=np.float32)\n",
    "    y_fit = np.empty(N_BINS, dtype=np.float32)\n",
    "\n",
    "    valid_bins = 0\n",
    "    for bi in range(N_BINS):\n",
    "        cnt = bin_cnt[bi]\n",
    "        x_fit[bi] = (bi + 0.5) / N_BINS * global_max  # bin center\n",
    "\n",
    "        if cnt <= 0:\n",
    "            y_fit[bi] = np.nan\n",
    "            continue\n",
    "\n",
    "        arr = bin_vals[bi, :cnt]\n",
    "        arr.sort()\n",
    "        qi = int(np.floor(Q * (cnt - 1)))\n",
    "        y_fit[bi] = arr[qi]\n",
    "        if np.isfinite(y_fit[bi]):\n",
    "            valid_bins += 1\n",
    "\n",
    "    if valid_bins < MIN_VALID_BINS:\n",
    "        return np.nan\n",
    "\n",
    "    return _linfit_s0(x_fit, y_fit)\n",
    "\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def _sd_map_pooled(cwd3d, x3d, year_slices_arr):\n",
    "    \"\"\"\n",
    "    cwd3d, x3d: (T, Y, X)\n",
    "    output: (Y, X) Sd (S0) estimated from pooled across years\n",
    "    \"\"\"\n",
    "    T, Y, X = cwd3d.shape\n",
    "    out = np.full((Y, X), np.nan, dtype=np.float32)\n",
    "\n",
    "    for j in nb.prange(Y):\n",
    "        for i in range(X):\n",
    "            out[j, i] = _s0_pooled_one_pixel(cwd3d[:, j, i], x3d[:, j, i], year_slices_arr)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_Sd(cwd_da, x_da, out_name):\n",
    "    Sd = xr.apply_ufunc(\n",
    "        _sd_map_pooled,\n",
    "        cwd_da,\n",
    "        x_da,\n",
    "        xr.DataArray(year_slices_arr, dims=(\"ny\", \"three\")),\n",
    "        input_core_dims=[[\"time\", \"lat\", \"lon\"], [\"time\", \"lat\", \"lon\"], [\"ny\", \"three\"]],\n",
    "        output_core_dims=[[\"lat\", \"lon\"]],\n",
    "        dask=\"parallelized\",\n",
    "        vectorize=False,\n",
    "        output_dtypes=[\"float32\"],\n",
    "    )\n",
    "    Sd.name = out_name\n",
    "    Sd = Sd.assign_coords(lat=cwd_da[\"lat\"], lon=cwd_da[\"lon\"])\n",
    "    return Sd\n",
    "\n",
    "\n",
    "SdEF  = compute_Sd(cwd, dEF,  \"SdEF\")\n",
    "SdSIF = compute_Sd(cwd, dSIF, \"SdSIF\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Write netCDF-4 (NC4)\n",
    "# -----------------------------\n",
    "enc2d = {\n",
    "    \"zlib\": True,\n",
    "    \"complevel\": 4,\n",
    "    \"dtype\": \"float32\",\n",
    "    \"chunksizes\": (200, 200),\n",
    "    \"_FillValue\": np.float32(np.nan),\n",
    "}\n",
    "\n",
    "xr.Dataset({\"SdEF\": SdEF}).to_netcdf(\n",
    "    OUT_SdEF, engine=\"netcdf4\", format=\"NETCDF4\", encoding={\"SdEF\": enc2d}\n",
    ")\n",
    "xr.Dataset({\"SdSIF\": SdSIF}).to_netcdf(\n",
    "    OUT_SdSIF, engine=\"netcdf4\", format=\"NETCDF4\", encoding={\"SdSIF\": enc2d}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
