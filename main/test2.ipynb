{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b425c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2003-01-01T00:00:00.000000000 -> 2020-12-26T00:00:00.000000000 len= 828\n",
      "lat: -89.975 89.975 n= 3600\n",
      "lon: -179.975 179.975 n= 7200\n",
      "N years: 18 first/last: [ 0 46] [782 828]\n",
      "Total tiles: 288 Example tile: (0, 300, 0, 300)\n",
      "[WARMUP] compiling numba on first tile...\n",
      "[WARMUP] done.\n",
      "[RUN] Start tiles: n=288 workers=24\n",
      "[PROGRESS] 10/288 done, fail=0\n",
      "[PROGRESS] 20/288 done, fail=0\n",
      "[PROGRESS] 30/288 done, fail=0\n",
      "[PROGRESS] 40/288 done, fail=0\n",
      "[PROGRESS] 50/288 done, fail=0\n",
      "[PROGRESS] 60/288 done, fail=0\n",
      "[PROGRESS] 70/288 done, fail=0\n",
      "[PROGRESS] 80/288 done, fail=0\n",
      "[PROGRESS] 90/288 done, fail=0\n",
      "[PROGRESS] 100/288 done, fail=0\n",
      "[PROGRESS] 110/288 done, fail=0\n",
      "[PROGRESS] 120/288 done, fail=0\n",
      "[PROGRESS] 130/288 done, fail=0\n",
      "[PROGRESS] 140/288 done, fail=0\n",
      "[PROGRESS] 150/288 done, fail=0\n",
      "[PROGRESS] 160/288 done, fail=0\n",
      "[PROGRESS] 170/288 done, fail=0\n",
      "[PROGRESS] 180/288 done, fail=0\n",
      "[PROGRESS] 190/288 done, fail=0\n",
      "[PROGRESS] 200/288 done, fail=0\n",
      "[PROGRESS] 210/288 done, fail=0\n",
      "[PROGRESS] 220/288 done, fail=0\n",
      "[PROGRESS] 230/288 done, fail=0\n",
      "[PROGRESS] 240/288 done, fail=0\n",
      "[PROGRESS] 250/288 done, fail=0\n",
      "[PROGRESS] 260/288 done, fail=0\n",
      "[PROGRESS] 270/288 done, fail=0\n",
      "[PROGRESS] 280/288 done, fail=0\n",
      "[DONE] tiles complete. fail= 0\n",
      "[WROTE] global: /tera04/zhwei/xionghui/bedrock/data/veg_activity/SdEF_QC.nc /tera04/zhwei/xionghui/bedrock/data/veg_activity/SdSIF_QC.nc\n",
      "{\n",
      "  \"SdEF_QC\": {\n",
      "    \"file\": \"/tera04/zhwei/xionghui/bedrock/data/veg_activity/SdEF_QC.nc\",\n",
      "    \"S0\": {\n",
      "      \"nan_frac\": 0.9566564043209876,\n",
      "      \"p01\": 29.870116710662842,\n",
      "      \"p50\": 247.51351928710938,\n",
      "      \"p99\": 3611.283386230477,\n",
      "      \"min\": 2.73056960105896,\n",
      "      \"max\": 87241.265625\n",
      "    },\n",
      "    \"flag\": {\n",
      "      \"frac_0_invalid\": 0.9277734182098766,\n",
      "      \"frac_1_linear\": 0.043343595679012345,\n",
      "      \"frac_2_segmented\": 0.028882986111111113\n",
      "    }\n",
      "  },\n",
      "  \"SdSIF_QC\": {\n",
      "    \"file\": \"/tera04/zhwei/xionghui/bedrock/data/veg_activity/SdSIF_QC.nc\",\n",
      "    \"S0\": {\n",
      "      \"nan_frac\": 0.9514699459876543,\n",
      "      \"p01\": 10.397302093505859,\n",
      "      \"p50\": 237.06895446777344,\n",
      "      \"p99\": 2933.0613769531265,\n",
      "      \"min\": 0.0011741684284061193,\n",
      "      \"max\": 30930.712890625\n",
      "    },\n",
      "    \"flag\": {\n",
      "      \"frac_0_invalid\": 0.9245678240740741,\n",
      "      \"frac_1_linear\": 0.048530054012345676,\n",
      "      \"frac_2_segmented\": 0.026902121913580247\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Full-run script: compute S0 with lightweight QC outputs (6 variables) for both dEF and dSIF.\n",
    "Based on your original cal_S0.py structure (tile + multiprocess + assemble).\n",
    "Outputs per dataset (SdEF / SdSIF):\n",
    "  - S0      (float32) : output S0 if linear accepted; NaN if invalid or segmented-preferred\n",
    "  - p_one   (float32) : one-sided p-value for slope b<0 (Normal approx)\n",
    "  - bic_lin (float32) : BIC of linear model\n",
    "  - bic_seg (float32) : BIC of 1-change-point piecewise model (NaN if not fitted)\n",
    "  - cp      (int16)   : change-point index in valid-bin sequence (-1 if none)\n",
    "  - flag    (uint8)   : 0 invalid; 1 linear accepted (S0 valid); 2 segmented better (flattening/change point)\n",
    "\n",
    "flag meaning:\n",
    "  0 = invalid / filtered\n",
    "  1 = linear accepted => S0 output\n",
    "  2 = segmented clearly better => classify change point / flattening => S0 not output\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ==========\n",
    "# Thread oversubscription control (important)\n",
    "# ==========\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"HDF5_USE_PATH_LOCKING\", \"FALSE\")\n",
    "\n",
    "# ======================\n",
    "# PATH CONFIG\n",
    "# ======================\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "ROOT_DIR = (NOTEBOOK_DIR / \"../data\").resolve()\n",
    "RUN_DIR = ROOT_DIR / \"run\"\n",
    "\n",
    "CWD_PATH  = RUN_DIR / \"CWD.nc\"\n",
    "dEF_PATH  = ROOT_DIR / \"ET/dEF.nc\"\n",
    "dSIF_PATH = ROOT_DIR / \"SIF/dSIF.nc\"\n",
    "\n",
    "OUT_GLOBAL_DIR = RUN_DIR / \"veg_activity\"\n",
    "OUT_GLOBAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TILE_DIR = RUN_DIR / \"veg_activity/tiles\"\n",
    "OUT_TILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# VARIABLE NAMES\n",
    "# ======================\n",
    "VAR_CWD  = \"CWD\"\n",
    "VAR_dEF  = \"dEF\"\n",
    "VAR_dSIF = \"dSIF\"\n",
    "\n",
    "# ======================\n",
    "# TILE CONFIG\n",
    "# ======================\n",
    "TILE_Y = 300\n",
    "TILE_X = 300\n",
    "\n",
    "# ======================\n",
    "# METHOD PARAMS\n",
    "# ======================\n",
    "NBINS = 50\n",
    "Q = 0.90\n",
    "\n",
    "MIN_EVENT_STEPS = 1       # 8-day data, 1 step = 8 days\n",
    "MINVALIDBINS = 8\n",
    "MINPOINTSTOTAL = 30\n",
    "RUNAWAY_YEARS = 5         # consecutive years without reset => discard\n",
    "\n",
    "# Missing value handling for dEF/dSIF (-9999 in your data)\n",
    "FILL_X = -9999.0\n",
    "\n",
    "# ===== QC hyper-parameters (tune if needed) =====\n",
    "ALPHA = 0.05              # one-sided p-value threshold for b<0\n",
    "BMIN  = 1e-8              # minimum |slope| to avoid exploding S0\n",
    "MIN_SEG_POINTS = 5        # min bins per segment for piecewise\n",
    "DELTA_BIC = 2.0           # segmented is \"clearly better\" if bic_seg + DELTA_BIC < bic_lin\n",
    "\n",
    "# ======================\n",
    "# NetCDF encoding\n",
    "# ======================\n",
    "ENC_F32_2D = dict(zlib=True, complevel=4, dtype=\"float32\")\n",
    "ENC_I16_2D = dict(zlib=True, complevel=4, dtype=\"int16\")\n",
    "ENC_U8_2D  = dict(zlib=True, complevel=4, dtype=\"uint8\")\n",
    "\n",
    "# ======================\n",
    "# Load coordinate template (single process)\n",
    "# ======================\n",
    "ds_cwd  = xr.open_dataset(CWD_PATH,  engine=\"netcdf4\")\n",
    "ds_def  = xr.open_dataset(dEF_PATH,  engine=\"netcdf4\")\n",
    "ds_dsif = xr.open_dataset(dSIF_PATH, engine=\"netcdf4\")\n",
    "\n",
    "time = ds_cwd[\"time\"].values\n",
    "lat  = ds_cwd[\"lat\"].values\n",
    "lon  = ds_cwd[\"lon\"].values\n",
    "\n",
    "assert ds_def[\"time\"].size == ds_cwd[\"time\"].size\n",
    "assert ds_dsif[\"time\"].size == ds_cwd[\"time\"].size\n",
    "assert ds_def[\"lat\"].size  == ds_cwd[\"lat\"].size\n",
    "assert ds_def[\"lon\"].size  == ds_cwd[\"lon\"].size\n",
    "\n",
    "print(\"time:\", time[0], \"->\", time[-1], \"len=\", len(time))\n",
    "print(\"lat:\", lat[0], lat[-1], \"n=\", len(lat))\n",
    "print(\"lon:\", lon[0], lon[-1], \"n=\", len(lon))\n",
    "\n",
    "# ======================\n",
    "# Build year slices\n",
    "# ======================\n",
    "def build_year_slices(time_index: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    return int32 array shape (n_years, 2) with [start, end) indices\n",
    "    \"\"\"\n",
    "    t = pd.to_datetime(time_index)\n",
    "    years = t.year.values\n",
    "    uniq_years = np.unique(years)\n",
    "    slices = []\n",
    "    for y in uniq_years:\n",
    "        idx = np.where(years == y)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        slices.append((int(idx[0]), int(idx[-1] + 1)))\n",
    "    return np.array(slices, dtype=np.int32)\n",
    "\n",
    "YEAR_SLICES = build_year_slices(time)\n",
    "print(\"N years:\", YEAR_SLICES.shape[0], \"first/last:\", YEAR_SLICES[0], YEAR_SLICES[-1])\n",
    "\n",
    "NY = len(lat)\n",
    "NX = len(lon)\n",
    "\n",
    "def make_tiles(ny, nx, tile_y, tile_x):\n",
    "    tiles = []\n",
    "    for y0 in range(0, ny, tile_y):\n",
    "        y1 = min(ny, y0 + tile_y)\n",
    "        for x0 in range(0, nx, tile_x):\n",
    "            x1 = min(nx, x0 + tile_x)\n",
    "            tiles.append((y0, y1, x0, x1))\n",
    "    return tiles\n",
    "\n",
    "TILES = make_tiles(NY, NX, TILE_Y, TILE_X)\n",
    "print(\"Total tiles:\", len(TILES), \"Example tile:\", TILES[0])\n",
    "\n",
    "# ======================\n",
    "# Numba core (QC version)\n",
    "# ======================\n",
    "import numba as nb\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _event_window_one_year(cwd_year, min_event_steps):\n",
    "    \"\"\"\n",
    "    Find the largest CWD event in one year.\n",
    "    Return (ok, start, end_exclusive, cwdmax).\n",
    "    Key fix vs your old code: start is reset_idx+1 to avoid including CWD<=0 reset point.\n",
    "    \"\"\"\n",
    "    Ty = cwd_year.size\n",
    "    peak = -1\n",
    "    cwdmax = -1e30\n",
    "    for i in range(Ty):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v > cwdmax:\n",
    "            cwdmax = v\n",
    "            peak = i\n",
    "    if peak < 0 or (not np.isfinite(cwdmax)) or cwdmax <= 0.0:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    # find last reset (CWD<=0) before peak, then +1\n",
    "    reset_idx = -1\n",
    "    for i in range(peak, -1, -1):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v <= 0.0:\n",
    "            reset_idx = i\n",
    "            break\n",
    "    if reset_idx >= 0:\n",
    "        start = reset_idx + 1\n",
    "        if start >= Ty:\n",
    "            return 0, 0, 0, 0.0\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    # end: first time after peak when CWD < 0.9*CWDmax\n",
    "    thr = 0.9 * cwdmax\n",
    "    end = Ty\n",
    "    for i in range(peak, Ty):\n",
    "        v = cwd_year[i]\n",
    "        if np.isfinite(v) and v < thr:\n",
    "            end = i\n",
    "            break\n",
    "\n",
    "    if end - start < min_event_steps:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    return 1, start, end, cwdmax\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _count_runaway_years(cwd, year_slices):\n",
    "    \"\"\"\n",
    "    Max consecutive years without any reset (CWD<=0).\n",
    "    \"\"\"\n",
    "    nY = year_slices.shape[0]\n",
    "    max_consec = 0\n",
    "    consec = 0\n",
    "    for yi in range(nY):\n",
    "        s = year_slices[yi, 0]\n",
    "        e = year_slices[yi, 1]\n",
    "        has_reset = False\n",
    "        for t in range(s, e):\n",
    "            v = cwd[t]\n",
    "            if np.isfinite(v) and v <= 0.0:\n",
    "                has_reset = True\n",
    "                break\n",
    "        if has_reset:\n",
    "            consec = 0\n",
    "        else:\n",
    "            consec += 1\n",
    "            if consec > max_consec:\n",
    "                max_consec = consec\n",
    "    return max_consec\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _norm_cdf(z):\n",
    "    # Phi(z) = 0.5 * erfc(-z/sqrt(2))\n",
    "    return 0.5 * math.erfc(-z / 1.4142135623730951)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _ols_fit(x, y):\n",
    "    \"\"\"\n",
    "    OLS fit y=a+b*x\n",
    "    returns (ok, a, b, rss, se_b, t_b, p_one_sided_for_b<0)\n",
    "    p-value uses Normal approximation (fast & numba-friendly).\n",
    "    \"\"\"\n",
    "    n = x.size\n",
    "    if n < 3:\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    sx = 0.0\n",
    "    sy = 0.0\n",
    "    sxx = 0.0\n",
    "    sxy = 0.0\n",
    "    for i in range(n):\n",
    "        sx += x[i]\n",
    "        sy += y[i]\n",
    "        sxx += x[i] * x[i]\n",
    "        sxy += x[i] * y[i]\n",
    "\n",
    "    denom = n * sxx - sx * sx\n",
    "    if denom == 0.0 or (not np.isfinite(denom)):\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    b = (n * sxy - sx * sy) / denom\n",
    "    a = (sy - b * sx) / n\n",
    "    if (not np.isfinite(a)) or (not np.isfinite(b)):\n",
    "        return 0, 0.0, 0.0, 1e30, 0.0, 0.0, 1.0\n",
    "\n",
    "    rss = 0.0\n",
    "    for i in range(n):\n",
    "        r = y[i] - (a + b * x[i])\n",
    "        rss += r * r\n",
    "    if (not np.isfinite(rss)) or rss <= 0.0:\n",
    "        rss = 1e-30\n",
    "\n",
    "    xbar = sx / n\n",
    "    sxxc = 0.0\n",
    "    for i in range(n):\n",
    "        dx = x[i] - xbar\n",
    "        sxxc += dx * dx\n",
    "    if sxxc <= 0.0 or (not np.isfinite(sxxc)):\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    s2 = rss / (n - 2)\n",
    "    if (not np.isfinite(s2)) or s2 <= 0.0:\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    se_b = math.sqrt(s2 / sxxc)\n",
    "    if (not np.isfinite(se_b)) or se_b <= 0.0:\n",
    "        return 0, a, b, rss, 0.0, 0.0, 1.0\n",
    "\n",
    "    t_b = b / se_b\n",
    "    p_one = _norm_cdf(t_b)  # one-sided for b<0\n",
    "    if not np.isfinite(p_one):\n",
    "        p_one = 1.0\n",
    "    return 1, a, b, rss, se_b, t_b, p_one\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _bic_from_rss(rss, n, k):\n",
    "    if rss <= 0.0:\n",
    "        rss = 1e-30\n",
    "    return n * math.log(rss / n) + k * math.log(n)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _best_piecewise_1cp_bic(x, y, min_seg_points):\n",
    "    \"\"\"\n",
    "    Best 1-change-point piecewise linear (bruteforce cp).\n",
    "    Returns (ok, cp, rss_total, bic_seg).\n",
    "    BIC uses k=5 (a1,b1,a2,b2,cp) conservative.\n",
    "    \"\"\"\n",
    "    n = x.size\n",
    "    if n < 2 * min_seg_points:\n",
    "        return 0, -1, 1e30, 1e30\n",
    "\n",
    "    best_rss = 1e30\n",
    "    best_cp = -1\n",
    "\n",
    "    for cp in range(min_seg_points, n - min_seg_points + 1):\n",
    "        ok1, a1, b1, rss1, _, _, _ = _ols_fit(x[:cp], y[:cp])\n",
    "        if ok1 == 0:\n",
    "            continue\n",
    "        ok2, a2, b2, rss2, _, _, _ = _ols_fit(x[cp:], y[cp:])\n",
    "        if ok2 == 0:\n",
    "            continue\n",
    "        rss = rss1 + rss2\n",
    "        if rss < best_rss:\n",
    "            best_rss = rss\n",
    "            best_cp = cp\n",
    "\n",
    "    if best_cp < 0:\n",
    "        return 0, -1, 1e30, 1e30\n",
    "\n",
    "    bic_seg = _bic_from_rss(best_rss, n, 5)\n",
    "    return 1, best_cp, best_rss, bic_seg\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _linfit_s0_qc(x, y, alpha, bmin):\n",
    "    \"\"\"\n",
    "    Linear fit + QC:\n",
    "      - a > 0\n",
    "      - b < 0\n",
    "      - one-sided p(b<0) < alpha\n",
    "      - |b| >= bmin\n",
    "    Returns (ok, s0, p_one, bic_lin)\n",
    "    \"\"\"\n",
    "    ok, a, b, rss, se_b, t_b, p_one = _ols_fit(x, y)\n",
    "    if ok == 0:\n",
    "        return 0, np.nan, np.nan, np.nan\n",
    "\n",
    "    n = x.size\n",
    "    bic_lin = _bic_from_rss(rss, n, 2)\n",
    "\n",
    "    if (not np.isfinite(a)) or a <= 0.0:\n",
    "        return 0, np.nan, np.float32(p_one), np.float32(bic_lin)\n",
    "    if (not np.isfinite(b)) or b >= 0.0:\n",
    "        return 0, np.nan, np.float32(p_one), np.float32(bic_lin)\n",
    "    if abs(b) < bmin:\n",
    "        return 0, np.nan, np.float32(p_one), np.float32(bic_lin)\n",
    "    if (not np.isfinite(p_one)) or (p_one >= alpha):\n",
    "        return 0, np.nan, np.float32(p_one), np.float32(bic_lin)\n",
    "\n",
    "    s0 = -a / b\n",
    "    if (not np.isfinite(s0)) or s0 <= 0.0:\n",
    "        return 0, np.nan, np.float32(p_one), np.float32(bic_lin)\n",
    "\n",
    "    return 1, np.float32(s0), np.float32(p_one), np.float32(bic_lin)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _s0_one_pixel_qc(\n",
    "    cwd, X, year_slices, nbins, q,\n",
    "    min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "    alpha, bmin, min_seg_points, delta_bic\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      s0(float32), p_one(float32), bic_lin(float32), bic_seg(float32), cp(int16), flag(uint8)\n",
    "    \"\"\"\n",
    "    # runaway check\n",
    "    if _count_runaway_years(cwd, year_slices) >= runaway_years:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    T = cwd.size\n",
    "    nY = year_slices.shape[0]\n",
    "\n",
    "    pool_cwd = np.empty(T, dtype=np.float32)\n",
    "    pool_x   = np.empty(T, dtype=np.float32)\n",
    "    n_pool = 0\n",
    "    gmax = 0.0\n",
    "\n",
    "    # per-year largest event\n",
    "    for yi in range(nY):\n",
    "        s = year_slices[yi, 0]\n",
    "        e = year_slices[yi, 1]\n",
    "        ok, st, ed, cwdmax = _event_window_one_year(cwd[s:e], min_event_steps)\n",
    "        if ok == 0:\n",
    "            continue\n",
    "\n",
    "        # pooling: keep only CWD>0 to avoid reset points\n",
    "        for tt in range(s + st, s + ed):\n",
    "            c = cwd[tt]\n",
    "            x = X[tt]\n",
    "            if np.isfinite(c) and np.isfinite(x) and c > 0.0:\n",
    "                pool_cwd[n_pool] = c\n",
    "                pool_x[n_pool] = x\n",
    "                n_pool += 1\n",
    "                if c > gmax:\n",
    "                    gmax = c\n",
    "\n",
    "    if n_pool < min_points_total or (not np.isfinite(gmax)) or gmax <= 0.0:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    # binning\n",
    "    counts = np.zeros(nbins, dtype=np.int32)\n",
    "    bin_id = np.empty(n_pool, dtype=np.int32)\n",
    "\n",
    "    for i in range(n_pool):\n",
    "        b = int(pool_cwd[i] / gmax * nbins)\n",
    "        if b < 0:\n",
    "            b = 0\n",
    "        elif b >= nbins:\n",
    "            b = nbins - 1\n",
    "        bin_id[i] = b\n",
    "        counts[b] += 1\n",
    "\n",
    "    offsets = np.empty(nbins, dtype=np.int32)\n",
    "    pos = 0\n",
    "    for b in range(nbins):\n",
    "        offsets[b] = pos\n",
    "        pos += counts[b]\n",
    "\n",
    "    xgrp = np.empty(n_pool, dtype=np.float32)\n",
    "    cursor = offsets.copy()\n",
    "    for i in range(n_pool):\n",
    "        b = bin_id[i]\n",
    "        p = cursor[b]\n",
    "        xgrp[p] = pool_x[i]\n",
    "        cursor[b] += 1\n",
    "\n",
    "    xb = np.empty(nbins, dtype=np.float32)\n",
    "    yqv = np.empty(nbins, dtype=np.float32)\n",
    "    n_valid = 0\n",
    "\n",
    "    for b in range(nbins):\n",
    "        n = counts[b]\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        st = offsets[b]\n",
    "        ed = st + n\n",
    "\n",
    "        xgrp[st:ed].sort()\n",
    "\n",
    "        k = int(math.ceil(q * n) - 1)\n",
    "        if k < 0:\n",
    "            k = 0\n",
    "        if k >= n:\n",
    "            k = n - 1\n",
    "\n",
    "        xq = xgrp[st + k]\n",
    "        if not np.isfinite(xq):\n",
    "            continue\n",
    "\n",
    "        center = (b + 0.5) / nbins * gmax\n",
    "        xb[n_valid] = center\n",
    "        yqv[n_valid] = xq\n",
    "        n_valid += 1\n",
    "\n",
    "    if n_valid < min_valid_bins:\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    x_use = xb[:n_valid]\n",
    "    y_use = yqv[:n_valid]\n",
    "\n",
    "    # (1) linear QC\n",
    "    ok, s0, p_one, bic_lin = _linfit_s0_qc(x_use, y_use, alpha, bmin)\n",
    "    if ok == 0:\n",
    "        # still provide p_one & bic_lin if possible\n",
    "        ok0, a0, b0, rss0, se_b0, t_b0, p0 = _ols_fit(x_use, y_use)\n",
    "        if ok0 == 1:\n",
    "            bic0 = _bic_from_rss(rss0, n_valid, 2)\n",
    "            return np.float32(np.nan), np.float32(p0), np.float32(bic0), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "        return np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.float32(np.nan), np.int16(-1), np.uint8(0)\n",
    "\n",
    "    # (2) segmented 1-cp BIC test\n",
    "    ok2, cp, rss_seg, bic_seg = _best_piecewise_1cp_bic(x_use, y_use, min_seg_points)\n",
    "    if ok2 == 1:\n",
    "        if bic_seg + delta_bic < bic_lin:\n",
    "            # segmented is clearly better => change point / flattening\n",
    "            return np.float32(np.nan), np.float32(p_one), np.float32(bic_lin), np.float32(bic_seg), np.int16(cp), np.uint8(2)\n",
    "        else:\n",
    "            return np.float32(s0), np.float32(p_one), np.float32(bic_lin), np.float32(bic_seg), np.int16(cp), np.uint8(1)\n",
    "\n",
    "    # no segmented fit\n",
    "    return np.float32(s0), np.float32(p_one), np.float32(bic_lin), np.float32(np.nan), np.int16(-1), np.uint8(1)\n",
    "\n",
    "\n",
    "@nb.njit(parallel=True, cache=True)\n",
    "def s0_tile_qc(\n",
    "    cwd3d, X3d, year_slices, nbins, q,\n",
    "    min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "    alpha, bmin, min_seg_points, delta_bic\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns 6 arrays (Y, X): s0, p_one, bic_lin, bic_seg, cp, flag\n",
    "    \"\"\"\n",
    "    T, Y, X = cwd3d.shape\n",
    "    s0_out = np.empty((Y, X), dtype=np.float32)\n",
    "    p_out  = np.empty((Y, X), dtype=np.float32)\n",
    "    bl_out = np.empty((Y, X), dtype=np.float32)\n",
    "    bs_out = np.empty((Y, X), dtype=np.float32)\n",
    "    cp_out = np.empty((Y, X), dtype=np.int16)\n",
    "    fg_out = np.empty((Y, X), dtype=np.uint8)\n",
    "\n",
    "    n_pix = Y * X\n",
    "    for p in nb.prange(n_pix):\n",
    "        j = p // X\n",
    "        i = p - j * X\n",
    "        s0, p_one, bic_lin, bic_seg, cp, flag = _s0_one_pixel_qc(\n",
    "            cwd3d[:, j, i], X3d[:, j, i],\n",
    "            year_slices, nbins, q,\n",
    "            min_event_steps, min_valid_bins, min_points_total, runaway_years,\n",
    "            alpha, bmin, min_seg_points, delta_bic\n",
    "        )\n",
    "        s0_out[j, i] = s0\n",
    "        p_out[j, i]  = p_one\n",
    "        bl_out[j, i] = bic_lin\n",
    "        bs_out[j, i] = bic_seg\n",
    "        cp_out[j, i] = cp\n",
    "        fg_out[j, i] = flag\n",
    "\n",
    "    return s0_out, p_out, bl_out, bs_out, cp_out, fg_out\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Tile IO helpers\n",
    "# ======================\n",
    "def write_tile_qc(out_path, lat_tile, lon_tile, prefix,\n",
    "                  s0, p_one, bic_lin, bic_seg, cp, flag):\n",
    "    \"\"\"\n",
    "    Write one tile file containing 6 variables.\n",
    "    \"\"\"\n",
    "    ds = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            S0=((\"lat\", \"lon\"), s0.astype(np.float32)),\n",
    "            p_one=((\"lat\", \"lon\"), p_one.astype(np.float32)),\n",
    "            bic_lin=((\"lat\", \"lon\"), bic_lin.astype(np.float32)),\n",
    "            bic_seg=((\"lat\", \"lon\"), bic_seg.astype(np.float32)),\n",
    "            cp=((\"lat\", \"lon\"), cp.astype(np.int16)),\n",
    "            flag=((\"lat\", \"lon\"), flag.astype(np.uint8)),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lat=(\"lat\", lat_tile),\n",
    "            lon=(\"lon\", lon_tile),\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            product=prefix,\n",
    "            NBINS=str(NBINS),\n",
    "            quantile=str(Q),\n",
    "            alpha=str(ALPHA),\n",
    "            bmin=str(BMIN),\n",
    "            min_seg_points=str(MIN_SEG_POINTS),\n",
    "            delta_bic=str(DELTA_BIC),\n",
    "            runaway_years=str(RUNAWAY_YEARS),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    enc = {\n",
    "        \"S0\": ENC_F32_2D,\n",
    "        \"p_one\": ENC_F32_2D,\n",
    "        \"bic_lin\": ENC_F32_2D,\n",
    "        \"bic_seg\": ENC_F32_2D,\n",
    "        \"cp\": ENC_I16_2D,\n",
    "        \"flag\": ENC_U8_2D,\n",
    "    }\n",
    "\n",
    "    ds.to_netcdf(out_path, engine=\"netcdf4\", format=\"NETCDF4\", encoding=enc)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def assemble_global_qc(pattern_prefix, out_path):\n",
    "    \"\"\"\n",
    "    Assemble global from tile QC files:\n",
    "      - expects tiles named like: {prefix}_y####-####_x####-####.nc\n",
    "    \"\"\"\n",
    "    files = sorted(OUT_TILE_DIR.glob(f\"{pattern_prefix}_y*-*_x*-*.nc\"))\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(f\"No tile files found for {pattern_prefix}\")\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", engine=\"netcdf4\")\n",
    "\n",
    "    # global encoding: choose chunksizes (lat,lon). adjust as needed\n",
    "    enc = {\n",
    "        \"S0\": dict(zlib=True, complevel=4, dtype=\"float32\", chunksizes=(600, 600)),\n",
    "        \"p_one\": dict(zlib=True, complevel=4, dtype=\"float32\", chunksizes=(600, 600)),\n",
    "        \"bic_lin\": dict(zlib=True, complevel=4, dtype=\"float32\", chunksizes=(600, 600)),\n",
    "        \"bic_seg\": dict(zlib=True, complevel=4, dtype=\"float32\", chunksizes=(600, 600)),\n",
    "        \"cp\": dict(zlib=True, complevel=4, dtype=\"int16\", chunksizes=(600, 600)),\n",
    "        \"flag\": dict(zlib=True, complevel=4, dtype=\"uint8\", chunksizes=(600, 600)),\n",
    "    }\n",
    "\n",
    "    ds.to_netcdf(out_path, engine=\"netcdf4\", format=\"NETCDF4\", encoding=enc)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Worker\n",
    "# ======================\n",
    "def worker_one_tile(tile_id: int):\n",
    "    y0, y1, x0, x1 = TILES[tile_id]\n",
    "    tile_tag = f\"y{y0:04d}-{y1:04d}_x{x0:04d}-{x1:04d}\"\n",
    "\n",
    "    f_def = OUT_TILE_DIR / f\"SdEF_QC_{tile_tag}.nc\"\n",
    "    f_sif = OUT_TILE_DIR / f\"SdSIF_QC_{tile_tag}.nc\"\n",
    "\n",
    "    # checkpoint\n",
    "    if f_def.exists() and f_sif.exists():\n",
    "        return tile_id, \"SKIP\"\n",
    "\n",
    "    # Each process opens datasets independently (important)\n",
    "    ds_c = xr.open_dataset(CWD_PATH, engine=\"netcdf4\")\n",
    "    ds_e = xr.open_dataset(dEF_PATH, engine=\"netcdf4\")\n",
    "    ds_s = xr.open_dataset(dSIF_PATH, engine=\"netcdf4\")\n",
    "\n",
    "    cwd  = ds_c[VAR_CWD].isel(lat=slice(y0, y1), lon=slice(x0, x1)).transpose(\"time\", \"lat\", \"lon\").load().data.astype(np.float32)\n",
    "\n",
    "    dEF  = ds_e[VAR_dEF].isel(lat=slice(y0, y1), lon=slice(x0, x1)).transpose(\"time\", \"lat\", \"lon\").load()\n",
    "    dSIF = ds_s[VAR_dSIF].isel(lat=slice(y0, y1), lon=slice(x0, x1)).transpose(\"time\", \"lat\", \"lon\").load()\n",
    "\n",
    "    # fill -> NaN\n",
    "    dEF  = dEF.where(dEF != FILL_X).data.astype(np.float32)\n",
    "    dSIF = dSIF.where(dSIF != FILL_X).data.astype(np.float32)\n",
    "\n",
    "    # compute QC outputs (6 vars)\n",
    "    s0_def, p_def, bl_def, bs_def, cp_def, fg_def = s0_tile_qc(\n",
    "        cwd, dEF, YEAR_SLICES, NBINS, Q,\n",
    "        MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS,\n",
    "        ALPHA, BMIN, MIN_SEG_POINTS, DELTA_BIC\n",
    "    )\n",
    "    s0_sif, p_sif, bl_sif, bs_sif, cp_sif, fg_sif = s0_tile_qc(\n",
    "        cwd, dSIF, YEAR_SLICES, NBINS, Q,\n",
    "        MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS,\n",
    "        ALPHA, BMIN, MIN_SEG_POINTS, DELTA_BIC\n",
    "    )\n",
    "\n",
    "    lat_tile = lat[y0:y1]\n",
    "    lon_tile = lon[x0:x1]\n",
    "\n",
    "    write_tile_qc(f_def, lat_tile, lon_tile, \"SdEF_QC\", s0_def, p_def, bl_def, bs_def, cp_def, fg_def)\n",
    "    write_tile_qc(f_sif, lat_tile, lon_tile, \"SdSIF_QC\", s0_sif, p_sif, bl_sif, bs_sif, cp_sif, fg_sif)\n",
    "\n",
    "    return tile_id, \"OK\"\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: quick compile on first tile to pay numba compile cost once\n",
    "    y0, y1, x0, x1 = TILES[0]\n",
    "    print(\"[WARMUP] compiling numba on first tile...\")\n",
    "    ds_c = xr.open_dataset(CWD_PATH, engine=\"netcdf4\")\n",
    "    ds_e = xr.open_dataset(dEF_PATH, engine=\"netcdf4\")\n",
    "\n",
    "    cwd0 = ds_c[VAR_CWD].isel(lat=slice(y0, y1), lon=slice(x0, x1)).transpose(\"time\", \"lat\", \"lon\").load().data.astype(np.float32)\n",
    "    x0da = ds_e[VAR_dEF].isel(lat=slice(y0, y1), lon=slice(x0, x1)).transpose(\"time\", \"lat\", \"lon\").load()\n",
    "    x0arr = x0da.where(x0da != FILL_X).data.astype(np.float32)\n",
    "\n",
    "    _ = s0_tile_qc(\n",
    "        cwd0, x0arr, YEAR_SLICES, NBINS, Q,\n",
    "        MIN_EVENT_STEPS, MINVALIDBINS, MINPOINTSTOTAL, RUNAWAY_YEARS,\n",
    "        ALPHA, BMIN, MIN_SEG_POINTS, DELTA_BIC\n",
    "    )\n",
    "    print(\"[WARMUP] done.\")\n",
    "\n",
    "    # ===== Parallel run =====\n",
    "    N_WORKERS = 24\n",
    "    done = 0\n",
    "    fail = 0\n",
    "\n",
    "    print(f\"[RUN] Start tiles: n={len(TILES)} workers={N_WORKERS}\")\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futs = {ex.submit(worker_one_tile, tid): tid for tid in range(len(TILES))}\n",
    "        for fut in as_completed(futs):\n",
    "            tid = futs[fut]\n",
    "            try:\n",
    "                tid2, status = fut.result()\n",
    "                done += 1\n",
    "                if status not in (\"OK\", \"SKIP\"):\n",
    "                    fail += 1\n",
    "                if done % 10 == 0:\n",
    "                    print(f\"[PROGRESS] {done}/{len(TILES)} done, fail={fail}\")\n",
    "            except Exception as e:\n",
    "                fail += 1\n",
    "                print(f\"[FAIL] tile {tid}: {repr(e)}\")\n",
    "\n",
    "    print(\"[DONE] tiles complete. fail=\", fail)\n",
    "    if fail > 0:\n",
    "        print(\"[WARN] Some tiles failed; fix failures before assembling globals.\")\n",
    "\n",
    "    # ===== Assemble globals =====\n",
    "    out_def = assemble_global_qc(\"SdEF_QC\", OUT_GLOBAL_DIR / \"SdEF_QC.nc\")\n",
    "    out_sif = assemble_global_qc(\"SdSIF_QC\", OUT_GLOBAL_DIR / \"SdSIF_QC.nc\")\n",
    "    print(\"[WROTE] global:\", out_def, out_sif)\n",
    "\n",
    "    # ===== quick QC summary =====\n",
    "    def qc_report(path):\n",
    "        ds = xr.open_dataset(path)\n",
    "        s0 = ds[\"S0\"].values\n",
    "        fg = ds[\"flag\"].values\n",
    "        rep = {\n",
    "            \"file\": str(path),\n",
    "            \"S0\": {\n",
    "                \"nan_frac\": float(np.isnan(s0).mean()),\n",
    "                \"p01\": float(np.nanpercentile(s0, 1)),\n",
    "                \"p50\": float(np.nanpercentile(s0, 50)),\n",
    "                \"p99\": float(np.nanpercentile(s0, 99)),\n",
    "                \"min\": float(np.nanmin(s0)),\n",
    "                \"max\": float(np.nanmax(s0)),\n",
    "            },\n",
    "            \"flag\": {\n",
    "                \"frac_0_invalid\": float((fg == 0).mean()),\n",
    "                \"frac_1_linear\": float((fg == 1).mean()),\n",
    "                \"frac_2_segmented\": float((fg == 2).mean()),\n",
    "            }\n",
    "        }\n",
    "        return rep\n",
    "\n",
    "    qc = {\n",
    "        \"SdEF_QC\": qc_report(out_def),\n",
    "        \"SdSIF_QC\": qc_report(out_sif),\n",
    "    }\n",
    "    print(json.dumps(qc, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
