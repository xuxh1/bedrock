{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6602d19c",
   "metadata": {},
   "source": [
    "# 1.Preprocess the MOD16A2GF_v6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51b96f",
   "metadata": {},
   "source": [
    "### 1.1 Make the csv list, aggregate the data to global 8-days data and mask the FillValue and multiply the scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e55ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODIS数据处理器 - 简化版\n",
      "============================================================\n",
      "读取已有索引文件...\n",
      "总共找到 236532 条文件记录\n",
      "测试模式：只处理前 2 个日期\n",
      "准备处理 2 个日期\n",
      "处理目录: /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata处理目录: /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata\n",
      "\n",
      "切换到目录: /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata切换到目录: /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata\n",
      "\n",
      "[错误] 处理失败: '/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/_tmp/A2003001' is not in the subpath of '/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata' OR one path is relative and the other is absolute.\n",
      "[错误] 处理失败: '/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/_tmp/A2003009' is not in the subpath of '/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata' OR one path is relative and the other is absolute.\n",
      "============================================================\n",
      "处理完成!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODIS蒸散发数据处理脚本（简化版）\n",
    "将HDF格式数据转换为0.05度分辨率的NetCDF文件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shlex\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_shell_command(command):\n",
    "    \"\"\"运行shell命令，如果失败则打印错误信息\"\"\"\n",
    "    # 打印执行的命令\n",
    "    print(f\"[执行] {' '.join(shlex.quote(str(arg)) for arg in command)}\")\n",
    "    \n",
    "    # 运行命令\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # 检查是否成功\n",
    "    if result.returncode != 0:\n",
    "        print(f\"[错误] 命令执行失败:\\n{result.stderr[:2000]}\")\n",
    "        raise subprocess.CalledProcessError(result.returncode, command)\n",
    "    \n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def get_hdf_info(file_path):\n",
    "    \"\"\"从HDF文件名中提取信息\"\"\"\n",
    "    # 文件名格式示例：MOD16A2GF.A2021001.h10v05.061.2022293211312.hdf\n",
    "    pattern = r\"MOD16A2GF\\.A(\\d{4})(\\d{3})\\.(h\\d{2}v\\d{2})\\.(\\d{3})\\.(\\d+)\\.hdf$\"\n",
    "    \n",
    "    match = re.match(pattern, file_path.name)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    year = int(match.group(1))\n",
    "    day_of_year = int(match.group(2))\n",
    "    tile_id = match.group(3)\n",
    "    \n",
    "    # 计算具体日期\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    file_date = start_date + timedelta(days=day_of_year - 1)\n",
    "    \n",
    "    return {\n",
    "        \"path\": str(file_path),\n",
    "        \"year\": year,\n",
    "        \"doy\": day_of_year,\n",
    "        \"date\": file_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"datestr\": f\"A{year}{day_of_year:03d}\",\n",
    "        \"tile\": tile_id,\n",
    "        \"collection\": match.group(4),\n",
    "        \"production\": match.group(5),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_gdal_hdf_path(hdf_file_path):\n",
    "    \"\"\"构建GDAL能够识别的HDF文件路径格式\"\"\"\n",
    "    # 从完整路径中提取文件名\n",
    "    hdf_filename = Path(hdf_file_path).name\n",
    "    # 正确的GDAL HDF格式：EOS_GRID:\"文件名\":MOD_Grid_MOD16A2:ET_500m\n",
    "    return f'HDF4_EOS:EOS_GRID:\"{hdf_filename}\":MOD_Grid_MOD16A2:ET_500m'\n",
    "\n",
    "\n",
    "def process_one_date(datestr, date_files, output_dir, temp_dir):\n",
    "    \"\"\"处理单个日期的所有文件\"\"\"\n",
    "    \n",
    "    # 准备输出文件路径\n",
    "    output_dir = Path(output_dir)\n",
    "    temp_dir = Path(temp_dir) / datestr\n",
    "    \n",
    "    # 创建目录\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 定义输出文件名\n",
    "    raw_output = output_dir / f\"MOD16A2GF_{datestr}_ET_500m_0p05deg_raw.nc\"\n",
    "    final_output = output_dir / f\"MOD16A2GF_{datestr}_ET_500m_0p05deg_phys.nc\"\n",
    "    \n",
    "    # 如果最终文件已存在，跳过处理\n",
    "    if final_output.exists():\n",
    "        return f\"[跳过] {datestr}\"\n",
    "    \n",
    "    # 获取原始数据目录\n",
    "    raw_data_dir = Path(date_files[\"path\"].iloc[0]).parent\n",
    "    print(f\"处理目录: {raw_data_dir}\")\n",
    "    \n",
    "    # 步骤1: 创建输入文件列表\n",
    "    input_list = temp_dir / \"file_list.txt\"\n",
    "    with open(input_list, \"w\") as f:\n",
    "        for hdf_file in date_files[\"path\"].tolist():\n",
    "            # 构建GDAL能够识别的路径\n",
    "            gdal_path = get_gdal_hdf_path(hdf_file)\n",
    "            f.write(gdal_path + \"\\n\")\n",
    "    \n",
    "    # 步骤2: 创建虚拟镶嵌文件\n",
    "    vrt_file = temp_dir / f\"{datestr}_ET_500m.vrt\"\n",
    "    \n",
    "    # 在执行gdalbuildvrt之前，切换到HDF文件所在的目录\n",
    "    # 这样GDAL才能正确找到文件\n",
    "    original_dir = os.getcwd()  # 保存当前工作目录\n",
    "    try:\n",
    "        os.chdir(raw_data_dir)  # 切换到HDF文件目录\n",
    "        print(f\"切换到目录: {raw_data_dir}\")\n",
    "        \n",
    "        # 构建相对路径到临时文件列表\n",
    "        relative_input_list = Path(temp_dir).relative_to(raw_data_dir) / \"file_list.txt\"\n",
    "        relative_vrt_file = Path(temp_dir).relative_to(raw_data_dir) / f\"{datestr}_ET_500m.vrt\"\n",
    "        \n",
    "        run_shell_command([\n",
    "            \"gdalbuildvrt\",\n",
    "            \"-overwrite\",\n",
    "            \"-input_file_list\", str(relative_input_list),\n",
    "            str(relative_vrt_file)\n",
    "        ])\n",
    "    finally:\n",
    "        os.chdir(original_dir)  # 切换回原始目录\n",
    "    \n",
    "    # 步骤3: 重投影和重采样到0.05度分辨率\n",
    "    # 注意：这里使用临时目录中的VRT文件的绝对路径\n",
    "    run_shell_command([\n",
    "        \"gdalwarp\",\n",
    "        \"-overwrite\",\n",
    "        \"-t_srs\", \"EPSG:4326\",           # 转换为经纬度坐标\n",
    "        \"-te\", \"-180\", \"-90\", \"180\", \"90\",  # 全球范围\n",
    "        \"-tr\", \"0.05\", \"0.05\",          # 0.05度分辨率\n",
    "        \"-tap\",                         # 对齐到网格\n",
    "        \"-r\", \"average\",                # 使用平均法重采样\n",
    "        \"-multi\",                       # 使用多线程\n",
    "        \"-wo\", \"NUM_THREADS=4\",         # 4个工作线程\n",
    "        \"-ot\", \"Float32\",               # 输出32位浮点数\n",
    "        \"-of\", \"netCDF\",                # 输出NetCDF格式\n",
    "        str(vrt_file),                  # 输入VRT文件（使用绝对路径）\n",
    "        str(raw_output),                # 输出文件\n",
    "        \n",
    "        # 压缩设置\n",
    "        \"-co\", \"FORMAT=NC4\",\n",
    "        \"-co\", \"COMPRESS=DEFLATE\",\n",
    "        \"-co\", \"ZLEVEL=3\",\n",
    "    ])\n",
    "    \n",
    "    # 步骤4: 处理数据值（应用填充值和缩放因子）\n",
    "    # 打开NetCDF文件\n",
    "    dataset = xr.open_dataset(raw_output)\n",
    "    \n",
    "    # 获取数据\n",
    "    data = dataset[\"ET_500m\"]\n",
    "    \n",
    "    # 获取缩放因子，如果没有则使用0.1\n",
    "    scale = data.attrs.get(\"scale_factor\", 0.1)\n",
    "    \n",
    "    # 获取填充值\n",
    "    fill_value = (\n",
    "        data.attrs.get(\"_FillValue\") or\n",
    "        data.attrs.get(\"missing_value\") or\n",
    "        data.encoding.get(\"_FillValue\") or\n",
    "        32767  # 默认值\n",
    "    )\n",
    "    \n",
    "    # 处理数据\n",
    "    # 1. 将填充值替换为NaN\n",
    "    # 2. 应用缩放因子\n",
    "    data = data.where(data != fill_value)\n",
    "    data = data.astype(\"float32\") * float(scale)\n",
    "    \n",
    "    # 更新属性\n",
    "    data.attrs.update({\n",
    "        \"units\": \"mm/8day\",\n",
    "        \"long_name\": \"Evapotranspiration (MOD16A2GF), upscaled to 0.05deg\",\n",
    "        \"scale_applied\": float(scale),\n",
    "        \"fillvalue_masked\": int(fill_value),\n",
    "    })\n",
    "    \n",
    "    # 移除不需要的属性\n",
    "    for attr in [\"scale_factor\", \"add_offset\", \"_FillValue\", \"missing_value\", \"valid_range\"]:\n",
    "        data.attrs.pop(attr, None)\n",
    "    \n",
    "    # 保存回数据集\n",
    "    dataset[\"ET_500m\"] = data\n",
    "    \n",
    "    # 配置输出\n",
    "    encoding = {\n",
    "        \"ET_500m\": {\n",
    "            \"dtype\": \"float32\",\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": 1,\n",
    "            \"_FillValue\": np.nan,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存处理后的文件\n",
    "    dataset.to_netcdf(final_output, encoding=encoding)\n",
    "    dataset.close()\n",
    "    \n",
    "    # 清理临时文件\n",
    "    try:\n",
    "        input_list.unlink(missing_ok=True)\n",
    "        vrt_file.unlink(missing_ok=True)\n",
    "        temp_dir.rmdir()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return f\"[完成] {datestr}\"\n",
    "\n",
    "\n",
    "def scan_hdf_files(data_dir, index_file):\n",
    "    \"\"\"扫描目录中的HDF文件，创建索引\"\"\"\n",
    "    \n",
    "    print(f\"正在扫描目录: {data_dir}\")\n",
    "    \n",
    "    # 查找所有HDF文件\n",
    "    hdf_files = []\n",
    "    for file_path in Path(data_dir).rglob(\"MOD16A2GF.A*.hdf\"):\n",
    "        hdf_files.append(file_path)\n",
    "    \n",
    "    print(f\"找到 {len(hdf_files)} 个HDF文件\")\n",
    "    \n",
    "    # 提取文件信息\n",
    "    file_info_list = []\n",
    "    for file_path in hdf_files:\n",
    "        info = get_hdf_info(file_path)\n",
    "        if info:\n",
    "            file_info_list.append(info)\n",
    "    \n",
    "    # 创建数据框并排序\n",
    "    file_df = pd.DataFrame(file_info_list)\n",
    "    file_df = file_df.sort_values([\"datestr\", \"tile\"])\n",
    "    \n",
    "    # 保存到CSV\n",
    "    file_df.to_csv(index_file, index=False)\n",
    "    print(f\"索引已保存到: {index_file}\")\n",
    "    \n",
    "    return file_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    \n",
    "    # 配置路径\n",
    "    raw_data_dir = \"/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/rawdata\"\n",
    "    output_dir = \"/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/p05\"\n",
    "    temp_dir = \"/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/_tmp\"\n",
    "    index_file = \"/share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/mod16a2gf_index.csv\"\n",
    "    \n",
    "    # 创建必要目录\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 步骤1: 扫描文件并创建索引\n",
    "    if not Path(index_file).exists():\n",
    "        print(\"索引文件不存在，正在创建...\")\n",
    "        file_df = scan_hdf_files(raw_data_dir, index_file)\n",
    "    else:\n",
    "        print(\"读取已有索引文件...\")\n",
    "        file_df = pd.read_csv(index_file)\n",
    "    \n",
    "    print(f\"总共找到 {len(file_df)} 条文件记录\")\n",
    "    \n",
    "    # 步骤2: 按日期分组\n",
    "    date_groups = []\n",
    "    for datestr, group in file_df.groupby(\"datestr\"):\n",
    "        date_groups.append((datestr, group))\n",
    "    \n",
    "    # 按日期排序\n",
    "    date_groups.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # 测试模式：只处理前2个日期\n",
    "    test_mode = True\n",
    "    if test_mode:\n",
    "        date_groups = date_groups[:2]\n",
    "        print(f\"测试模式：只处理前 {len(date_groups)} 个日期\")\n",
    "    \n",
    "    print(f\"准备处理 {len(date_groups)} 个日期\")\n",
    "    \n",
    "    # 步骤3: 并行处理\n",
    "    # 确定使用的进程数（不超过4个）\n",
    "    max_workers = 4\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 提交所有任务\n",
    "        tasks = []\n",
    "        for datestr, date_files in date_groups:\n",
    "            task = executor.submit(\n",
    "                process_one_date,\n",
    "                datestr,\n",
    "                date_files,\n",
    "                output_dir,\n",
    "                temp_dir\n",
    "            )\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # 收集结果\n",
    "        for task in as_completed(tasks):\n",
    "            try:\n",
    "                result = task.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"[错误] 处理失败: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODIS数据处理器 - 简化版\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"处理完成!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28175c43",
   "metadata": {},
   "source": [
    "### 1.3 Mergetime the single timestep to year group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdo -O mergetime /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/0p05deg_phys/MOD16A2GF_A???????_ET_500m_0p05deg_phys.nc4 \\\n",
    "                /share/home/dq076/bedrock/data/ET/MOD16A2GF_v6.1/MOD16A2GF_ET_0p05deg_8day_2003_2020.nc4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
